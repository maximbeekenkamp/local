{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Operator Training Demo: CDON Dataset\n",
    "\n",
    "This notebook demonstrates end-to-end training of neural operator models (DeepONet, FNO, UNet) on the CDON dataset.\n",
    "\n",
    "**Features:**\n",
    "- Trains on **real CDON data**\n",
    "- **Sequential training with all 6 loss functions**:\n",
    "  - **BASELINE**: Relative L2 loss only (baseline MSE)\n",
    "  - **BSP**: MSE + fixed BSP loss with k\u00b2 weighting\n",
    "  - **Log-BSP**: MSE + BSP with log\u2081\u2080 spectral energies (uniform \u03bb_k weighting)\n",
    "  - **SA-BSP (Per-bin)**: MSE + 32 adaptive per-bin weights (negated gradients for frequency emphasis)\n",
    "  - **SA-BSP (Global)**: MSE + 2 adaptive weights (w_mse + w_bsp) for MSE/BSP balance\n",
    "  - **SA-BSP (Combined)**: MSE + 34 weights (w_mse + w_bsp + 32 per-bin) with full competitive dynamics\n",
    "- **Multi-loss comparison plots** showing training metrics\n",
    "- **Energy spectrum visualization** (E(k) vs wavenumber) to identify spectral bias\n",
    "- **Spectral bias quantification** with metrics and comparison plots\n",
    "- Compatible with Google Colab\n",
    "\n",
    "**Models available:**\n",
    "- `deeponet`: Branch-trunk architecture with SIREN activation (~235K params)\n",
    "- `fno`: Fourier Neural Operator (~261K params)\n",
    "- `unet`: Encoder-decoder with skip connections (~249K params)\n",
    "\n",
    "**SA-PINNs Implementation:**\n",
    "Uses saddle-point optimization with negated gradients (gradient ascent on loss) to enable competitive dynamics. This automatically emphasizes difficult frequency bins and finds optimal loss balance through min-max optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 0: Force Reload Modules\n",
    "\n",
    "Run this cell to reload all project modules after code changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force reload of all modules\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Get list of all loaded modules from the project\n",
    "modules_to_reload = []\n",
    "for module_name in list(sys.modules.keys()):\n",
    "    if any(x in module_name for x in ['src.', 'configs.']):\n",
    "        modules_to_reload.append(module_name)\n",
    "\n",
    "# Remove modules from sys.modules to force reload\n",
    "for module_name in modules_to_reload:\n",
    "    if module_name in sys.modules:\n",
    "        del sys.modules[module_name]\n",
    "\n",
    "print(f\"\u2713 Cleared {len(modules_to_reload)} cached modules\")\n",
    "print(\"  Run Cell 1 to reimport all modules with latest code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup & Imports (Colab-Ready)\n",
    "\n",
    "**\u26a0\ufe0f IMPORTANT - NumPy Compatibility**:\n",
    "If you get an AttributeError about `_blas_supports_fpe`, it means numpy needs to be reloaded:\n",
    "1. Go to Runtime \u2192 Restart Runtime\n",
    "2. Re-run this cell\n",
    "\n",
    "This only needs to be done once per session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab setup\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure we're in /content\n",
    "try:\n",
    "    os.chdir('/content')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Clone repository if running in Colab\n",
    "repo_path = Path('/content/local')\n",
    "if not repo_path.exists():\n",
    "    print(\"\ud83d\udce5 Cloning repository...\")\n",
    "    !git clone https://github.com/maximbeekenkamp/local.git\n",
    "    print(\"\u2705 Repository cloned\")\n",
    "else:\n",
    "    print(\"\ud83d\udce5 Updating repository...\")\n",
    "    !git -C /content/local pull\n",
    "    print(\"\u2705 Repository updated\")\n",
    "\n",
    "# Change to repo directory\n",
    "try:\n",
    "    os.chdir('/content/local')\n",
    "    print(f\"\u2705 Changed to: {os.getcwd()}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Install dependencies\n",
    "print(\"\\n\ud83d\udce6 Installing dependencies...\")\n",
    "!pip install -r requirements.txt -q\n",
    "print(\"\u2705 Dependencies installed\")\n",
    "\n",
    "# Check numpy version before importing\n",
    "import numpy as np\n",
    "print(f\"\\n\ud83d\udccc NumPy version: {np.__version__}\")\n",
    "if np.__version__.startswith('1.'):\n",
    "    print(\"\u26a0\ufe0f  WARNING: NumPy 1.x detected. You may need to restart runtime.\")\n",
    "    print(\"   Go to Runtime \u2192 Restart Runtime, then re-run this cell.\")\n",
    "\n",
    "# Standard imports\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Project imports - wrapped in try/except to catch numpy issues\n",
    "try:\n",
    "    from src.core.data_processing.cdon_dataset import CDONDataset\n",
    "    from src.core.data_processing.cdon_transforms import CDONNormalization\n",
    "    from src.core.models.model_factory import create_model\n",
    "    from src.core.training.simple_trainer import SimpleTrainer\n",
    "    from configs.training_config import TrainingConfig\n",
    "\n",
    "    print(\"\\n\u2713 Imports successful\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "except AttributeError as e:\n",
    "    if '_blas_supports_fpe' in str(e):\n",
    "        print(\"\\n\u274c NumPy binary incompatibility detected!\")\n",
    "        print(\"\\n\ud83d\udd27 FIX: Go to Runtime \u2192 Restart Runtime, then re-run this cell.\")\n",
    "        print(\"   (This happens when numpy is upgraded but old binaries are still loaded)\")\n",
    "        raise\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Configuration\n",
    "\n",
    "Configure optional features before loading data:\n",
    "- **Causal padding**: Zero-padding preprocessing (Reference CausalityDeepONet)\n",
    "- **DeepONet activation**: Choose activation function (REQU, TANH, RELU, SIREN)\n",
    "- **Penalty loss**: Optional inverse-variance weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# 1. CAUSALITY: Zero-padding preprocessing (Reference CausalityDeepONet)\n",
    "USE_CAUSAL_PADDING = True  # ENABLED BY DEFAULT (matches reference)\n",
    "# Set to False to disable causal padding (standard preprocessing)\n",
    "\n",
    "# 2. DEEPONET ACTIVATION: Choose activation function\n",
    "DEEPONET_ACTIVATION = 'requ'  # Options: 'requ' (default), 'tanh', 'relu', 'siren'\n",
    "# 'requ' = ReLU\u00b2 (reference default, smooth gradients)\n",
    "# 'tanh' = Stable for operator learning\n",
    "# 'relu' = Standard ReLU\n",
    "# 'siren' = Sinusoidal activation (requires siren-pytorch)\n",
    "\n",
    "# 3. PENALTY LOSS: Optional inverse-variance weighting\n",
    "USE_PENALTY_LOSS = False  # Set to True to enable penalty weighting\n",
    "PENALTY_EPSILON = 1e-8     # Numerical stability for penalty\n",
    "PENALTY_PER_SAMPLE = True  # Per-sample (True) or global (False) penalty\n",
    "\n",
    "print(\"\u2713 Configuration loaded:\")\n",
    "print(f\"  Causal padding:     {'ENABLED' if USE_CAUSAL_PADDING else 'DISABLED'}\")\n",
    "print(f\"  DeepONet activation: {DEEPONET_ACTIVATION.upper()}\")\n",
    "print(f\"  Penalty loss:       {'ENABLED' if USE_PENALTY_LOSS else 'DISABLED'}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD REAL CDON DATA\n",
    "# ============================================================================\n",
    "\n",
    "# Get project root\n",
    "project_root = Path.cwd()\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Data directory\n",
    "DATA_DIR = project_root / 'CDONData'\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "\n",
    "# Create normalization object (required by CDONDataset)\n",
    "stats_path = project_root / 'configs' / 'cdon_stats.json'\n",
    "print(f\"Loading stats from: {stats_path}\")\n",
    "normalizer = CDONNormalization(stats_path=str(stats_path))\n",
    "\n",
    "# Create datasets with optional causal padding\n",
    "train_dataset = CDONDataset(\n",
    "    data_dir=str(DATA_DIR),\n",
    "    split='train',\n",
    "    normalize=normalizer,\n",
    "    mode='sequence',  # Use sequence mode for BSP loss training\n",
    "    use_causal_sequence=USE_CAUSAL_PADDING,  # Apply causal padding if enabled\n",
    "    signal_length=4000\n",
    ")\n",
    "\n",
    "val_dataset = CDONDataset(\n",
    "    data_dir=str(DATA_DIR),\n",
    "    split='test',\n",
    "    normalize=normalizer,\n",
    "    mode='sequence',  # Use sequence mode for BSP loss training\n",
    "    use_causal_sequence=USE_CAUSAL_PADDING,  # Apply causal padding if enabled\n",
    "    signal_length=4000\n",
    ")\n",
    "\n",
    "# Create dataloaders with optimized batch sizes\n",
    "# Colab memory constraint: Keep total RAM usage < 10GB\n",
    "# BSP loss does FFT on PREDICTIONS (target FFT is cached)\n",
    "# With 567K param model, even batch=4 sequences can OOM\n",
    "# Per-timestep: [batch, 1, 4000] inputs + activations\n",
    "# Sequence: [batch, 1, 4000] predictions need FFT + binning\n",
    "BATCH_SIZE_PER_TIMESTEP = 32   # For MSE on per-timestep data (Colab-safe)\n",
    "BATCH_SIZE_SEQUENCE = 2        # For BSP on sequences (ultra-conservative for 567K model)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE_SEQUENCE,  # Sequence data uses smaller batch\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Use 0 for Colab to avoid RAM multiplication\n",
    "    pin_memory=False  # Disable for Colab compatibility\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE_SEQUENCE,  # Sequence data uses smaller batch\n",
    "    shuffle=False,\n",
    "    num_workers=0,  # Use 0 for Colab to avoid RAM multiplication\n",
    "    pin_memory=False  # Disable for Colab compatibility\n",
    ")\n",
    "\n",
    "print(f\"\\n\u2713 Data loaded successfully\")\n",
    "print(f\"  Train samples: {len(train_dataset)}\")\n",
    "print(f\"  Val samples: {len(val_dataset)}\")\n",
    "print(f\"  Batch size (per-timestep): {BATCH_SIZE_PER_TIMESTEP}\")\n",
    "print(f\"  Batch size (sequence): {BATCH_SIZE_SEQUENCE}\")\n",
    "\n",
    "# Inspect a sample\n",
    "sample = train_dataset[0]\n",
    "if len(sample) == 3:\n",
    "    sample_input, sample_target, sample_idx = sample\n",
    "else:\n",
    "    sample_input, sample_target = sample\n",
    "print(f\"\\nSample shapes:\")\n",
    "print(f\"  Input: {sample_input.shape}\")\n",
    "print(f\"  Target: {sample_target.shape}\")\n",
    "\n",
    "if USE_CAUSAL_PADDING:\n",
    "    expected_input_len = 4000 + (4000 - 1)  # signal_length + padding\n",
    "    if sample_input.shape[-1] == expected_input_len:\n",
    "        print(f\"  \u2713 Causal padding applied correctly (input length: {expected_input_len})\")\n",
    "    else:\n",
    "        print(f\"  \u26a0 Warning: Expected input length {expected_input_len}, got {sample_input.shape[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3.5: Loss Variant Configuration (Quick Switch)\n",
    "\n",
    "**Easily switch between loss variants by changing `LOSS_VARIANT`:**\n",
    "\n",
    "Available variants:\n",
    "- `'bsp'`: Fixed BSP with k\u00b2 weighting (static spectral loss)\n",
    "- `'log_bsp'`: Log-domain BSP with uniform weighting (wide dynamic range)\n",
    "- `'sa_bsp'`: Self-Adaptive BSP with trainable per-bin weights (emphasis on difficult frequencies)\n",
    "- `'sa_log_bsp'`: Self-Adaptive Log-BSP (combines adaptive weights with log-domain energies)\n",
    "\n",
    "Each variant can use different adaptation modes (per-bin, global, combined) via `SA_ADAPT_MODE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOSS VARIANT CONFIGURATION - Change this to switch between loss types\n",
    "# ============================================================================\n",
    "\n",
    "# Choose loss variant\n",
    "LOSS_VARIANT = 'bsp'  # Options: 'bsp', 'log_bsp', 'sa_bsp', 'sa_log_bsp'\n",
    "\n",
    "# For SA-BSP variants, choose adaptation mode\n",
    "SA_ADAPT_MODE = 'per-bin'  # Options: 'per-bin', 'global', 'combined'\n",
    "# 'per-bin': 32 trainable weights (one per frequency bin)\n",
    "# 'global': 2 trainable weights (w_mse + w_bsp for MSE/BSP balance)\n",
    "# 'combined': 34 trainable weights (w_mse + w_bsp + 32 per-bin)\n",
    "\n",
    "# Common parameters for all loss variants\n",
    "N_BINS = 32\n",
    "SIGNAL_LENGTH = 4000\n",
    "CACHE_PATH = 'cache/true_spectrum.npz'\n",
    "EPSILON = 1e-8\n",
    "\n",
    "# Loss variant configurations\n",
    "LOSS_CONFIGS = {\n",
    "    # Fixed BSP with k\u00b2 weighting\n",
    "    'bsp': {\n",
    "        'loss_type': 'combined',\n",
    "        'loss_params': {\n",
    "            'base_loss': 'mse',\n",
    "            'spectral_loss': 'bsp',\n",
    "            'mu': 1.0,\n",
    "            'n_bins': N_BINS,\n",
    "            'epsilon': EPSILON,\n",
    "            'binning_mode': 'linear',\n",
    "            'signal_length': SIGNAL_LENGTH,\n",
    "            'cache_path': CACHE_PATH,\n",
    "            'lambda_k_mode': 'k_squared',  # Static k\u00b2 weighting for turbulence\n",
    "            'use_log': False,              # Standard energy (not log)\n",
    "            'use_output_norm': True,       # Per-batch output normalization\n",
    "            'use_minmax_norm': True,       # Per-sample min-max normalization\n",
    "            'loss_type': 'mspe'            # Mean Squared Percentage Error\n",
    "        },\n",
    "        'description': 'MSE + BSP (\u03bc=1.0, \u03bb_k=k\u00b2) with normalization - Fixed spectral loss'\n",
    "    },\n",
    "    \n",
    "    # Log-domain BSP with uniform weighting\n",
    "    'log_bsp': {\n",
    "        'loss_type': 'combined',\n",
    "        'loss_params': {\n",
    "            'base_loss': 'mse',\n",
    "            'spectral_loss': 'bsp',\n",
    "            'mu': 1.0,\n",
    "            'n_bins': N_BINS,\n",
    "            'epsilon': EPSILON,\n",
    "            'binning_mode': 'linear',\n",
    "            'signal_length': SIGNAL_LENGTH,\n",
    "            'cache_path': CACHE_PATH,\n",
    "            'lambda_k_mode': 'uniform',    # Uniform \u03bb_k = 1 for all bins\n",
    "            'use_log': True,               # Log\u2081\u2080 transform of energies\n",
    "            'use_output_norm': True,       # Per-batch output normalization\n",
    "            'use_minmax_norm': True,       # Per-sample min-max normalization\n",
    "            'loss_type': 'l2_norm'         # L2 norm loss\n",
    "        },\n",
    "        'description': 'MSE + Log-BSP: log\u2081\u2080(E) with uniform weighting - Wide dynamic range'\n",
    "    },\n",
    "    \n",
    "    # Self-Adaptive BSP (trainable weights)\n",
    "    'sa_bsp': {\n",
    "        'loss_type': 'combined',\n",
    "        'loss_params': {\n",
    "            'base_loss': 'mse',\n",
    "            'spectral_loss': 'sa_bsp',\n",
    "            'n_bins': N_BINS,\n",
    "            'adapt_mode': SA_ADAPT_MODE,   # Controlled by SA_ADAPT_MODE variable above\n",
    "            'init_weight': 1.0,\n",
    "            'epsilon': 1e-6,               # Increased from 1e-8 per paper ablation\n",
    "            'binning_mode': 'linear',\n",
    "            'signal_length': SIGNAL_LENGTH,\n",
    "            'cache_path': CACHE_PATH,\n",
    "            'lambda_k_mode': 'k_squared',  # k\u00b2 initialization for trainable weights\n",
    "            'use_log': False,              # Standard energy (not log)\n",
    "            'use_output_norm': True,       # Per-batch output normalization\n",
    "            'use_minmax_norm': True,       # Per-sample min-max normalization\n",
    "            'loss_type': 'mspe'            # Mean Squared Percentage Error\n",
    "        },\n",
    "        'description': f'MSE + SA-BSP ({SA_ADAPT_MODE}): Trainable \u03bb_k weights (init: k\u00b2) - Adaptive emphasis'\n",
    "    },\n",
    "    \n",
    "    # Self-Adaptive Log-BSP (trainable weights + log-domain)\n",
    "    'sa_log_bsp': {\n",
    "        'loss_type': 'combined',\n",
    "        'loss_params': {\n",
    "            'base_loss': 'mse',\n",
    "            'spectral_loss': 'sa_bsp',\n",
    "            'n_bins': N_BINS,\n",
    "            'adapt_mode': SA_ADAPT_MODE,   # Controlled by SA_ADAPT_MODE variable above\n",
    "            'init_weight': 1.0,\n",
    "            'epsilon': 1e-6,               # Increased from 1e-8 per paper ablation\n",
    "            'binning_mode': 'linear',\n",
    "            'signal_length': SIGNAL_LENGTH,\n",
    "            'cache_path': CACHE_PATH,\n",
    "            'lambda_k_mode': 'uniform',    # Uniform initialization for log-domain\n",
    "            'use_log': True,               # Log\u2081\u2080 transform of energies\n",
    "            'use_output_norm': True,       # Per-batch output normalization\n",
    "            'use_minmax_norm': True,       # Per-sample min-max normalization\n",
    "            'loss_type': 'l2_norm'         # L2 norm loss\n",
    "        },\n",
    "        'description': f'MSE + SA-Log-BSP ({SA_ADAPT_MODE}): Trainable weights + log\u2081\u2080(E) - Adaptive + wide range'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Get selected loss configuration\n",
    "from configs.loss_config import LossConfig\n",
    "selected_loss_config = LossConfig.from_dict(LOSS_CONFIGS[LOSS_VARIANT])\n",
    "\n",
    "print(\"\u2713 Loss variant configuration loaded:\")\n",
    "print(f\"  Variant: {LOSS_VARIANT.upper()}\")\n",
    "if 'sa_' in LOSS_VARIANT:\n",
    "    print(f\"  SA Mode: {SA_ADAPT_MODE.upper()}\")\n",
    "print(f\"  Description: {selected_loss_config.description}\")\n",
    "print(f\"\\n  Parameters:\")\n",
    "for key, value in selected_loss_config.loss_params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3.7: Spectrum Comparison Utilities\n",
    "\n",
    "Helper functions for comparing energy spectra across different loss variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SPECTRUM COMPARISON UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def compare_loss_variants(\n",
    "    trained_models_dict, \n",
    "    val_loader, \n",
    "    loss_variants=['bsp', 'log_bsp', 'sa_bsp', 'sa_log_bsp'],\n",
    "    model_arch='deeponet',\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare energy spectra across different loss variants.\n",
    "    \n",
    "    Args:\n",
    "        trained_models_dict: Dictionary of trained models {variant: model}\n",
    "        val_loader: Validation data loader\n",
    "        loss_variants: List of loss variant names to compare\n",
    "        model_arch: Model architecture name\n",
    "        device: Device for inference\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of spectra {variant: {frequencies, energy_median, energy_p16, energy_p84}}\n",
    "    \"\"\"\n",
    "    from src.core.visualization.spectral_analysis import compute_unbinned_spectrum, compute_cached_true_spectrum\n",
    "    from configs.visualization_config import SPECTRUM_CACHE_FILENAME, CACHE_DIR\n",
    "    \n",
    "    print(\"Computing energy spectra for loss variant comparison...\")\n",
    "    \n",
    "    # Load true spectrum from cache\n",
    "    cache_path = f'{CACHE_DIR}/{SPECTRUM_CACHE_FILENAME}'\n",
    "    print(f\"Loading true spectrum from cache: {cache_path}\")\n",
    "    cached = np.load(cache_path)\n",
    "    k_true = cached['unbinned_frequencies']\n",
    "    E_true_median = cached['unbinned_energy_median']\n",
    "    E_true_p16 = cached['unbinned_energy_p16']\n",
    "    E_true_p84 = cached['unbinned_energy_p84']\n",
    "    print(f\"\u2713 True spectrum loaded ({len(k_true)} frequencies)\")\n",
    "    \n",
    "    spectra = {}\n",
    "    \n",
    "    # Store true spectrum\n",
    "    spectra['True'] = {\n",
    "        'frequencies': k_true,\n",
    "        'energy_median': E_true_median,\n",
    "        'energy_p16': E_true_p16,\n",
    "        'energy_p84': E_true_p84\n",
    "    }\n",
    "    \n",
    "    # Compute spectra for each loss variant\n",
    "    for variant in loss_variants:\n",
    "        key = f\"{model_arch}_{variant}\"\n",
    "        \n",
    "        if key not in trained_models_dict:\n",
    "            print(f\"  \u26a0\ufe0f  Skipping {variant}: model key '{key}' not found\")\n",
    "            continue\n",
    "        \n",
    "        model = trained_models_dict[key]\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        \n",
    "        try:\n",
    "            # Collect predictions from all validation batches\n",
    "            all_preds = []\n",
    "            print(f\"  Processing {variant.upper()}...\", end='')\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for val_input, _ in val_loader:\n",
    "                    val_input = val_input.to(device)\n",
    "                    pred = model(val_input)\n",
    "                    all_preds.append(pred.cpu())\n",
    "            \n",
    "            # Stack predictions\n",
    "            all_preds_tensor = torch.cat(all_preds, dim=0)\n",
    "            \n",
    "            # Compute unbinned spectrum with percentile-based uncertainty\n",
    "            k_pred, E_pred_median, E_pred_p16, E_pred_p84 = compute_unbinned_spectrum(all_preds_tensor)\n",
    "            \n",
    "            spectra[variant] = {\n",
    "                'frequencies': k_pred,\n",
    "                'energy_median': E_pred_median,\n",
    "                'energy_p16': E_pred_p16,\n",
    "                'energy_p84': E_pred_p84\n",
    "            }\n",
    "            \n",
    "            print(f\" \u2713 ({all_preds_tensor.shape[0]} samples)\")\n",
    "        except Exception as e:\n",
    "            print(f\" \u274c Error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n\u2713 Spectra computed for {len(spectra)} entries\\n\")\n",
    "    return spectra\n",
    "\n",
    "\n",
    "def plot_spectrum_comparison(\n",
    "    spectra,\n",
    "    variants_to_plot=['bsp', 'log_bsp', 'sa_bsp', 'sa_log_bsp'],\n",
    "    title_suffix=\"\",\n",
    "    figsize=(14, 9)\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot energy spectrum comparison for selected loss variants.\n",
    "    \n",
    "    Args:\n",
    "        spectra: Dictionary of spectra from compare_loss_variants()\n",
    "        variants_to_plot: List of variant names to include in plot\n",
    "        title_suffix: Additional text for plot title\n",
    "        figsize: Figure size tuple\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Color scheme\n",
    "    colors = {\n",
    "        'True': '#000000',\n",
    "        'bsp': '#ff7f0e',\n",
    "        'log_bsp': '#2ca02c',\n",
    "        'sa_bsp': '#d62728',\n",
    "        'sa_log_bsp': '#9467bd'\n",
    "    }\n",
    "    \n",
    "    # Plot ground truth\n",
    "    if 'True' in spectra:\n",
    "        data = spectra['True']\n",
    "        k = data['frequencies']\n",
    "        E_median = data['energy_median']\n",
    "        E_p16 = data['energy_p16']\n",
    "        E_p84 = data['energy_p84']\n",
    "        \n",
    "        ax.loglog(k, E_median, color=colors['True'], linewidth=3, \n",
    "                 label='True (Real Data)', zorder=10, alpha=0.9)\n",
    "        ax.fill_between(k, E_p16, E_p84,\n",
    "                        color=colors['True'], alpha=0.15, zorder=9,\n",
    "                        label='True (16th-84th percentile)')\n",
    "    \n",
    "    # Plot model predictions\n",
    "    label_map = {\n",
    "        'bsp': 'BSP (k\u00b2 weighting)',\n",
    "        'log_bsp': 'Log-BSP (uniform \u03bb)',\n",
    "        'sa_bsp': 'SA-BSP (adaptive \u03bb)',\n",
    "        'sa_log_bsp': 'SA-Log-BSP (adaptive + log)'\n",
    "    }\n",
    "    \n",
    "    for variant in variants_to_plot:\n",
    "        if variant not in spectra:\n",
    "            print(f\"  \u26a0\ufe0f  Skipping {variant}: not in spectra dictionary\")\n",
    "            continue\n",
    "        \n",
    "        data = spectra[variant]\n",
    "        k = data['frequencies']\n",
    "        E_median = data['energy_median']\n",
    "        E_p16 = data['energy_p16']\n",
    "        E_p84 = data['energy_p84']\n",
    "        \n",
    "        color = colors.get(variant, '#888888')\n",
    "        label = label_map.get(variant, variant.upper())\n",
    "        \n",
    "        # Plot median line\n",
    "        ax.loglog(k, E_median, color=color, linewidth=2.5, \n",
    "                 alpha=0.85, label=label, zorder=5)\n",
    "        \n",
    "        # Plot uncertainty band\n",
    "        ax.fill_between(k, E_p16, E_p84,\n",
    "                        color=color, alpha=0.12, zorder=4)\n",
    "    \n",
    "    # Configure plot\n",
    "    ax.set_xlabel('Frequency (normalized)', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('E(k) - Spectral Power', fontsize=14, fontweight='bold')\n",
    "    ax.set_title(f'Energy Spectrum Comparison: Loss Variants{title_suffix}', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    ax.legend(fontsize=11, loc='best', framealpha=0.95)\n",
    "    ax.grid(True, alpha=0.3, which='both', linestyle='--')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\u2713 Spectrum comparison plot complete\")\n",
    "\n",
    "\n",
    "def plot_side_by_side_spectra(\n",
    "    spectra_dict_1,\n",
    "    spectra_dict_2,\n",
    "    label_1=\"Model 1\",\n",
    "    label_2=\"Model 2\",\n",
    "    variants=['bsp', 'log_bsp', 'sa_bsp', 'sa_log_bsp'],\n",
    "    figsize=(20, 9)\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot two spectrum comparisons side-by-side.\n",
    "    \n",
    "    Args:\n",
    "        spectra_dict_1: First spectra dictionary\n",
    "        spectra_dict_2: Second spectra dictionary\n",
    "        label_1: Label for first model\n",
    "        label_2: Label for second model\n",
    "        variants: List of variants to plot\n",
    "        figsize: Figure size tuple\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    colors = {\n",
    "        'True': '#000000',\n",
    "        'bsp': '#ff7f0e',\n",
    "        'log_bsp': '#2ca02c',\n",
    "        'sa_bsp': '#d62728',\n",
    "        'sa_log_bsp': '#9467bd'\n",
    "    }\n",
    "    \n",
    "    label_map = {\n",
    "        'bsp': 'BSP (k\u00b2)',\n",
    "        'log_bsp': 'Log-BSP',\n",
    "        'sa_bsp': 'SA-BSP',\n",
    "        'sa_log_bsp': 'SA-Log-BSP'\n",
    "    }\n",
    "    \n",
    "    for ax, spectra, title in [(ax1, spectra_dict_1, label_1), (ax2, spectra_dict_2, label_2)]:\n",
    "        # Plot ground truth\n",
    "        if 'True' in spectra:\n",
    "            data = spectra['True']\n",
    "            ax.loglog(data['frequencies'], data['energy_median'], \n",
    "                     color=colors['True'], linewidth=3, label='True', zorder=10)\n",
    "            ax.fill_between(data['frequencies'], data['energy_p16'], data['energy_p84'],\n",
    "                           color=colors['True'], alpha=0.15, zorder=9)\n",
    "        \n",
    "        # Plot variants\n",
    "        for variant in variants:\n",
    "            if variant in spectra:\n",
    "                data = spectra[variant]\n",
    "                color = colors.get(variant, '#888888')\n",
    "                label = label_map.get(variant, variant)\n",
    "                \n",
    "                ax.loglog(data['frequencies'], data['energy_median'],\n",
    "                         color=color, linewidth=2.5, alpha=0.85, label=label, zorder=5)\n",
    "                ax.fill_between(data['frequencies'], data['energy_p16'], data['energy_p84'],\n",
    "                               color=color, alpha=0.12, zorder=4)\n",
    "        \n",
    "        ax.set_xlabel('Frequency', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('E(k)', fontsize=12, fontweight='bold')\n",
    "        ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "        ax.legend(fontsize=10, loc='best')\n",
    "        ax.grid(True, alpha=0.3, which='both', linestyle='--')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\u2713 Side-by-side spectrum comparison complete\")\n",
    "\n",
    "\n",
    "print(\"\u2713 Spectrum comparison utilities loaded\")\n",
    "print(\"\\nAvailable functions:\")\n",
    "print(\"  \u2022 compare_loss_variants(trained_models_dict, val_loader, loss_variants, ...)\")\n",
    "print(\"  \u2022 plot_spectrum_comparison(spectra, variants_to_plot, ...)\")\n",
    "print(\"  \u2022 plot_side_by_side_spectra(spectra_1, spectra_2, label_1, label_2, ...)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3.9: Usage Example - Quick Loss Variant Comparison\n",
    "\n",
    "**Example workflow for comparing loss variants:**\n",
    "\n",
    "1. **Train with different variants** by changing `LOSS_VARIANT` in Cell 3.5:\n",
    "   ```python\n",
    "   # Train BSP\n",
    "   LOSS_VARIANT = 'bsp'\n",
    "   # Run training cells...\n",
    "   \n",
    "   # Train Log-BSP\n",
    "   LOSS_VARIANT = 'log_bsp'\n",
    "   # Run training cells...\n",
    "   \n",
    "   # Train SA-BSP\n",
    "   LOSS_VARIANT = 'sa_bsp'\n",
    "   SA_ADAPT_MODE = 'per-bin'\n",
    "   # Run training cells...\n",
    "   ```\n",
    "\n",
    "2. **Compare spectra** using the utility functions:\n",
    "   ```python\n",
    "   # After training multiple variants, compare their spectra\n",
    "   spectra = compare_loss_variants(\n",
    "       trained_models, \n",
    "       val_loader, \n",
    "       loss_variants=['bsp', 'log_bsp', 'sa_bsp'],\n",
    "       model_arch='deeponet'\n",
    "   )\n",
    "   \n",
    "   # Plot comparison\n",
    "   plot_spectrum_comparison(\n",
    "       spectra, \n",
    "       variants_to_plot=['bsp', 'log_bsp', 'sa_bsp'],\n",
    "       title_suffix=' - DeepONet Model'\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. **Side-by-side comparison** for different model architectures:\n",
    "   ```python\n",
    "   # Compare FNO vs DeepONet with same loss variant\n",
    "   plot_side_by_side_spectra(\n",
    "       spectra_fno, \n",
    "       spectra_deeponet,\n",
    "       label_1=\"FNO + BSP\",\n",
    "       label_2=\"DeepONet + BSP\",\n",
    "       variants=['bsp', 'log_bsp']\n",
    "   )\n",
    "   ```\n",
    "\n",
    "**Quick switches:**\n",
    "- Change `LOSS_VARIANT` to switch between 'bsp', 'log_bsp', 'sa_bsp', 'sa_log_bsp'\n",
    "- Change `SA_ADAPT_MODE` for SA-BSP variants: 'per-bin', 'global', 'combined'\n",
    "- Change `MODEL_ARCH` to try different architectures: 'deeponet', 'fno', 'unet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NOTE: Configuration has been moved to Cell 2 (above)\n",
    "# ============================================================================\n",
    "# This cell is kept for backward compatibility but is no longer needed.\n",
    "# All configuration (USE_CAUSAL_PADDING, DEEPONET_ACTIVATION, etc.) \n",
    "# is now defined in Cell 2 before data loading.\n",
    "\n",
    "print(\"\u26a0\ufe0f  This cell is deprecated - configuration is now in Cell 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model architecture\n",
    "MODEL_ARCH = 'deeponet'  # Options: 'deeponet', 'fno', 'unet'\n",
    "\n",
    "# Create model with optional DeepONet activation\n",
    "if MODEL_ARCH == 'deeponet':\n",
    "    model = create_model(MODEL_ARCH, config={'activation': DEEPONET_ACTIVATION})\n",
    "    print(f\"\u2713 Created {MODEL_ARCH.upper()} model with {DEEPONET_ACTIVATION.upper()} activation\")\n",
    "else:\n",
    "    model = create_model(MODEL_ARCH)\n",
    "    print(f\"\u2713 Created {MODEL_ARCH.upper()} model\")\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"  Parameters: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Choose Model Architecture\n",
    "\n",
    "**Change `MODEL_ARCH` to try different models:**\n",
    "- `'deeponet'`: Branch-trunk architecture with SIREN activation\n",
    "- `'fno'`: Fourier Neural Operator\n",
    "- `'unet'`: U-Net encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model architecture\n",
    "MODEL_ARCH = 'deeponet'  # Options: 'deeponet', 'fno', 'unet'\n",
    "\n",
    "# Create model with optional DeepONet activation\n",
    "if MODEL_ARCH == 'deeponet':\n",
    "    model = create_model(MODEL_ARCH, config={'activation': DEEPONET_ACTIVATION})\n",
    "    print(f\"\u2713 Created {MODEL_ARCH.upper()} model with {DEEPONET_ACTIVATION.upper()} activation\")\n",
    "else:\n",
    "    model = create_model(MODEL_ARCH)\n",
    "    print(f\"\u2713 Created {MODEL_ARCH.upper()} model\")\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"  Parameters: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Initialize Results Storage\n",
    "\n",
    "We'll train with all 6 loss types sequentially and store results for comparison:\n",
    "- **BASELINE**: Relative L2 loss only (MSE baseline)\n",
    "- **BSP**: MSE + fixed BSP loss with k\u00b2 weighting\n",
    "- **Log-BSP**: MSE + BSP with log\u2081\u2080 spectral energies (uniform weighting)\n",
    "- **SA-BSP-PERBIN**: MSE + 32 adaptive per-bin weights (negated gradients for frequency emphasis)\n",
    "- **SA-BSP-GLOBAL**: 2 adaptive weights (w_mse + w_bsp) with negated gradients for MSE/BSP balance\n",
    "- **SA-BSP-COMBINED**: 34 weights (w_mse + w_bsp + 32 per-bin) with full competitive dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4.5: Force Module Reload\n",
    "\n",
    "Run this cell to reload trainer and loss modules. This ensures any code changes are picked up.\n",
    "\n",
    "**When to run:**\n",
    "- After modifying trainer or loss code\n",
    "- Before starting training runs\n",
    "- If you see unexpected behavior (e.g., baseline showing DUAL-BATCH mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force reload of training and evaluation modules\n",
    "import sys\n",
    "\n",
    "modules_to_reload = [\n",
    "    'src.core.training.simple_trainer',\n",
    "    'src.core.evaluation.loss_factory',\n",
    "    'src.core.evaluation.binned_spectral_loss',\n",
    "    'src.core.evaluation.adaptive_spectral_loss',\n",
    "]\n",
    "\n",
    "for module_name in modules_to_reload:\n",
    "    if module_name in sys.modules:\n",
    "        del sys.modules[module_name]\n",
    "\n",
    "from src.core.training.simple_trainer import SimpleTrainer\n",
    "from src.core.evaluation.loss_factory import create_loss\n",
    "\n",
    "print(\"\u2713 Modules reloaded - trainer will use latest code\")\n",
    "print(\"  Run training cells below to see updated behavior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import loss configurations\n",
    "from configs.loss_config import (\n",
    "    BASELINE_CONFIG, \n",
    "    BSP_CONFIG,\n",
    "    LOG_BSP_CONFIG,\n",
    "    SA_BSP_PERBIN_CONFIG,\n",
    "    SA_BSP_GLOBAL_CONFIG,\n",
    "    SA_BSP_COMBINED_CONFIG\n",
    ")\n",
    "from src.core.evaluation.loss_factory import create_loss\n",
    "\n",
    "# Loss configuration map\n",
    "loss_config_map = {\n",
    "    'baseline': BASELINE_CONFIG,\n",
    "    'bsp': BSP_CONFIG,\n",
    "    'log-bsp': LOG_BSP_CONFIG,\n",
    "    'sa-bsp-perbin': SA_BSP_PERBIN_CONFIG,\n",
    "    'sa-bsp-global': SA_BSP_GLOBAL_CONFIG,\n",
    "    'sa-bsp-combined': SA_BSP_COMBINED_CONFIG\n",
    "}\n",
    "\n",
    "# Storage dictionaries for results from all loss types\n",
    "all_training_results = {}  # Key: f\"{MODEL_ARCH}_{loss_type}\"\n",
    "all_trainers = {}\n",
    "trained_models = {}\n",
    "\n",
    "print(\"\u2713 Storage initialized for multi-loss training\")\n",
    "print(\"\\nWill train with 6 loss types:\")\n",
    "print(\"  1. BASELINE:\", BASELINE_CONFIG.description)\n",
    "print(\"  2. BSP:\", BSP_CONFIG.description)\n",
    "print(\"  3. LOG-BSP:\", LOG_BSP_CONFIG.description)\n",
    "print(\"  4. SA-BSP-PERBIN:\", SA_BSP_PERBIN_CONFIG.description)\n",
    "print(\"  5. SA-BSP-GLOBAL:\", SA_BSP_GLOBAL_CONFIG.description)\n",
    "print(\"  6. SA-BSP-COMBINED:\", SA_BSP_COMBINED_CONFIG.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Sequential Training with All Loss Types\n",
    "\n",
    "Train the same model architecture with all 6 loss functions sequentially:\n",
    "1. **BASELINE** - Pure MSE baseline\n",
    "2. **BSP** - Fixed spectral loss with k\u00b2 weighting\n",
    "3. **Log-BSP** - Spectral loss with log\u2081\u2080 energies and uniform weighting\n",
    "4. **SA-BSP-PERBIN** - 32 adaptive weights (emphasize hard frequency bins)\n",
    "5. **SA-BSP-GLOBAL** - 2 adaptive weights (learn MSE/BSP balance)\n",
    "6. **SA-BSP-COMBINED** - 34 adaptive weights (full competitive dynamics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with all 6 loss types sequentially\n",
    "loss_types_to_train = ['baseline', 'bsp', 'log-bsp', 'sa-bsp-perbin', 'sa-bsp-global', 'sa-bsp-combined']\n",
    "\n",
    "for LOSS_TYPE in loss_types_to_train:\n",
    "    # Clear GPU cache before each training run to prevent OOM\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training {MODEL_ARCH.upper()} with {LOSS_TYPE.upper()} Loss\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    # Select loss configuration\n",
    "    selected_loss_config = loss_config_map[LOSS_TYPE]\n",
    "    print(f\"Loss config: {selected_loss_config.description}\")\n",
    "\n",
    "    # Create loss function\n",
    "    criterion = create_loss(selected_loss_config)\n",
    "    print(f\"\u2713 Loss function created: {type(criterion).__name__}\")\n",
    "\n",
    "    # Create FRESH model for this loss type (important!)\n",
    "    model_for_loss = create_model(MODEL_ARCH)\n",
    "    num_params = sum(p.numel() for p in model_for_loss.parameters() if p.requires_grad)\n",
    "    print(f\"\u2713 Fresh model created ({num_params:,} parameters)\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # AUTOMATIC LOADER SELECTION: DeepONet always uses dual-batch\n",
    "    # ========================================================================\n",
    "\n",
    "    # Determine which loaders are needed based on model and loss\n",
    "    # DeepONet: Always use dual-batch (even for baseline, to handle dictionary format)\n",
    "    # FNO/UNet: Sequence-only\n",
    "    use_dual_batch = (MODEL_ARCH == 'deeponet')\n",
    "\n",
    "    if use_dual_batch:\n",
    "        # DeepONet + BSP/SA-BSP: DUAL-BATCH training\n",
    "        # MSE uses per-timestep (320K samples), BSP uses sequences (80 samples)\n",
    "        print(f\"\\n\ud83d\udcca Creating dual-batch loaders (per-timestep + sequence)...\")\n",
    "\n",
    "        # Create per-timestep datasets for MSE component\n",
    "        per_ts_train_dataset = CDONDataset(\n",
    "            data_dir=str(DATA_DIR),\n",
    "            split='train',\n",
    "            normalize=normalizer,\n",
    "            mode='per_timestep',\n",
    "            use_causal_sequence=USE_CAUSAL_PADDING,\n",
    "            signal_length=4000\n",
    "        )\n",
    "        per_ts_val_dataset = CDONDataset(\n",
    "            data_dir=str(DATA_DIR),\n",
    "            split='test',\n",
    "            normalize=normalizer,\n",
    "            mode='per_timestep',\n",
    "            use_causal_sequence=USE_CAUSAL_PADDING,\n",
    "            signal_length=4000\n",
    "        )\n",
    "\n",
    "        # Create sequence datasets for BSP component\n",
    "        seq_train_dataset = CDONDataset(\n",
    "            data_dir=str(DATA_DIR),\n",
    "            split='train',\n",
    "            normalize=normalizer,\n",
    "            mode='sequence',\n",
    "            use_causal_sequence=False,  # Sequences never use causal padding\n",
    "            signal_length=4000\n",
    "        )\n",
    "        seq_val_dataset = CDONDataset(\n",
    "            data_dir=str(DATA_DIR),\n",
    "            split='test',\n",
    "            normalize=normalizer,\n",
    "            mode='sequence',\n",
    "            use_causal_sequence=False,\n",
    "            signal_length=4000\n",
    "        )\n",
    "\n",
    "        # Create per-timestep loaders\n",
    "        per_ts_train_loader = DataLoader(\n",
    "            per_ts_train_dataset,\n",
    "            batch_size=BATCH_SIZE_PER_TIMESTEP,  # Large batch for per-timestep\n",
    "            shuffle=True,\n",
    "            num_workers=0,  # Use 0 for Colab to avoid RAM multiplication\n",
    "            pin_memory=False  # Disable for Colab compatibility\n",
    "        )\n",
    "        per_ts_val_loader = DataLoader(\n",
    "            per_ts_val_dataset,\n",
    "            batch_size=BATCH_SIZE_PER_TIMESTEP,  # Large batch for per-timestep\n",
    "            shuffle=False,\n",
    "            num_workers=0,  # Use 0 for Colab to avoid RAM multiplication\n",
    "            pin_memory=False  # Disable for Colab compatibility\n",
    "        )\n",
    "\n",
    "        # Create sequence loaders\n",
    "        seq_train_loader = DataLoader(\n",
    "            seq_train_dataset,\n",
    "            batch_size=BATCH_SIZE_SEQUENCE,  # Smaller batch for sequences\n",
    "            shuffle=True,\n",
    "            num_workers=0,  # Use 0 for Colab to avoid RAM multiplication\n",
    "            pin_memory=False  # Disable for Colab compatibility\n",
    "        )\n",
    "        seq_val_loader = DataLoader(\n",
    "            seq_val_dataset,\n",
    "            batch_size=BATCH_SIZE_SEQUENCE,  # Smaller batch for sequences\n",
    "            shuffle=False,\n",
    "            num_workers=0,  # Use 0 for Colab to avoid RAM multiplication\n",
    "            pin_memory=False  # Disable for Colab compatibility\n",
    "        )\n",
    "\n",
    "        print(f\"  \u2713 Per-timestep train: {len(per_ts_train_dataset):,} samples\")\n",
    "        print(f\"  \u2713 Per-timestep val:   {len(per_ts_val_dataset):,} samples\")\n",
    "        print(f\"  \u2713 Sequence train:     {len(seq_train_dataset)} samples\")\n",
    "        print(f\"  \u2713 Sequence val:       {len(seq_val_dataset)} samples\")\n",
    "\n",
    "    elif MODEL_ARCH == 'deeponet' and LOSS_TYPE == 'baseline':\n",
    "        # DeepONet + baseline: DUAL-BATCH (MSE only, but needs both loaders for trainer)\n",
    "        # Note: BSP weight is 0 for baseline, but we use dual-batch infrastructure\n",
    "        # to correctly handle dictionary format from per-timestep loaders\n",
    "        print(f\"\\n\ud83d\udcca Creating dual-batch loaders (per-timestep + sequence)...\")\n",
    "\n",
    "        # Create per-timestep datasets for MSE component\n",
    "        per_ts_train_dataset = CDONDataset(\n",
    "            data_dir=str(DATA_DIR),\n",
    "            split='train',\n",
    "            normalize=normalizer,\n",
    "            mode='per_timestep',\n",
    "            use_causal_sequence=USE_CAUSAL_PADDING,\n",
    "            signal_length=4000\n",
    "        )\n",
    "        per_ts_val_dataset = CDONDataset(\n",
    "            data_dir=str(DATA_DIR),\n",
    "            split='test',\n",
    "            normalize=normalizer,\n",
    "            mode='per_timestep',\n",
    "            use_causal_sequence=USE_CAUSAL_PADDING,\n",
    "            signal_length=4000\n",
    "        )\n",
    "\n",
    "        # Create sequence datasets for dual-batch infrastructure\n",
    "        seq_train_dataset = CDONDataset(\n",
    "            data_dir=str(DATA_DIR),\n",
    "            split='train',\n",
    "            normalize=normalizer,\n",
    "            mode='sequence',\n",
    "            use_causal_sequence=False,  # Sequences never use causal padding\n",
    "            signal_length=4000\n",
    "        )\n",
    "        seq_val_dataset = CDONDataset(\n",
    "            data_dir=str(DATA_DIR),\n",
    "            split='test',\n",
    "            normalize=normalizer,\n",
    "            mode='sequence',\n",
    "            use_causal_sequence=False,\n",
    "            signal_length=4000\n",
    "        )\n",
    "\n",
    "        # Create per-timestep loaders\n",
    "        per_ts_train_loader = DataLoader(\n",
    "            per_ts_train_dataset,\n",
    "            batch_size=BATCH_SIZE_PER_TIMESTEP,  # Large batch for per-timestep\n",
    "            shuffle=True,\n",
    "            num_workers=0,  # Use 0 for Colab to avoid RAM multiplication\n",
    "            pin_memory=False  # Disable for Colab compatibility\n",
    "        )\n",
    "        per_ts_val_loader = DataLoader(\n",
    "            per_ts_val_dataset,\n",
    "            batch_size=BATCH_SIZE_PER_TIMESTEP,  # Large batch for per-timestep\n",
    "            shuffle=False,\n",
    "            num_workers=0,  # Use 0 for Colab to avoid RAM multiplication\n",
    "            pin_memory=False  # Disable for Colab compatibility\n",
    "        )\n",
    "\n",
    "        # Create sequence loaders\n",
    "        seq_train_loader = DataLoader(\n",
    "            seq_train_dataset,\n",
    "            batch_size=BATCH_SIZE_SEQUENCE,  # Smaller batch for sequences\n",
    "            shuffle=True,\n",
    "            num_workers=0,  # Use 0 for Colab to avoid RAM multiplication\n",
    "            pin_memory=False  # Disable for Colab compatibility\n",
    "        )\n",
    "        seq_val_loader = DataLoader(\n",
    "            seq_val_dataset,\n",
    "            batch_size=BATCH_SIZE_SEQUENCE,  # Smaller batch for sequences\n",
    "            shuffle=False,\n",
    "            num_workers=0,  # Use 0 for Colab to avoid RAM multiplication\n",
    "            pin_memory=False  # Disable for Colab compatibility\n",
    "        )\n",
    "\n",
    "        print(f\"  \u2713 Per-timestep train: {len(per_ts_train_dataset):,} samples\")\n",
    "        print(f\"  \u2713 Per-timestep val:   {len(per_ts_val_dataset):,} samples\")\n",
    "        print(f\"  \u2713 Sequence train:     {len(seq_train_dataset)} samples\")\n",
    "        print(f\"  \u2713 Sequence val:       {len(seq_val_dataset)} samples\")\n",
    "\n",
    "    else:\n",
    "        # FNO/UNet: SEQUENCE only (all losses)\n",
    "        print(f\"\\n\ud83d\udcca Creating sequence loaders ({MODEL_ARCH.upper()} architecture)...\")\n",
    "\n",
    "        seq_train_dataset = CDONDataset(\n",
    "            data_dir=str(DATA_DIR),\n",
    "            split='train',\n",
    "            normalize=normalizer,\n",
    "            mode='sequence',\n",
    "            use_causal_sequence=False,\n",
    "            signal_length=4000\n",
    "        )\n",
    "        seq_val_dataset = CDONDataset(\n",
    "            data_dir=str(DATA_DIR),\n",
    "            split='test',\n",
    "            normalize=normalizer,\n",
    "            mode='sequence',\n",
    "            use_causal_sequence=False,\n",
    "            signal_length=4000\n",
    "        )\n",
    "\n",
    "        seq_train_loader = DataLoader(\n",
    "            seq_train_dataset,\n",
    "            batch_size=BATCH_SIZE_SEQUENCE,  # Sequence batch size\n",
    "            shuffle=True,\n",
    "            num_workers=0,  # Use 0 for Colab to avoid RAM multiplication\n",
    "            pin_memory=False  # Disable for Colab compatibility\n",
    "        )\n",
    "        seq_val_loader = DataLoader(\n",
    "            seq_val_dataset,\n",
    "            batch_size=BATCH_SIZE_SEQUENCE,  # Sequence batch size\n",
    "            shuffle=False,\n",
    "            num_workers=0,  # Use 0 for Colab to avoid RAM multiplication\n",
    "            pin_memory=False  # Disable for Colab compatibility\n",
    "        )\n",
    "\n",
    "        print(f\"  \u2713 Sequence train: {len(seq_train_dataset)} samples\")\n",
    "        print(f\"  \u2713 Sequence val:   {len(seq_val_dataset)} samples\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # CREATE TRAINER with appropriate loader API\n",
    "    # ========================================================================\n",
    "\n",
    "    # Create training config\n",
    "    optimizer_type = 'adam' if MODEL_ARCH == 'fno' else 'soap'\n",
    "\n",
    "    config = TrainingConfig(\n",
    "        num_epochs=50,\n",
    "        learning_rate=1e-3,\n",
    "        optimizer_type=optimizer_type,\n",
    "        batch_size=BATCH_SIZE_SEQUENCE,  # Config parameter (not directly used by dataloaders)\n",
    "        weight_decay=1e-4,\n",
    "        scheduler_type='cosine',\n",
    "        cosine_eta_min=1e-6,\n",
    "        eval_metrics=['mse', 'spectrum_error'],\n",
    "        eval_frequency=1,\n",
    "        checkpoint_dir=f'checkpoints/{MODEL_ARCH}_{LOSS_TYPE}',\n",
    "        save_best=False,\n",
    "        save_latest=False,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        num_workers=0,  # Use 0 for Colab to avoid RAM multiplication\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Create trainer with appropriate loader API\n",
    "    if use_dual_batch:\n",
    "        # Dual-batch API: pass both per-timestep and sequence loaders\n",
    "        # For DeepONet (includes baseline, BSP, SA-BSP)\n",
    "        trainer = SimpleTrainer(\n",
    "            model=model_for_loss,\n",
    "            per_timestep_train_loader=per_ts_train_loader,\n",
    "            sequence_train_loader=seq_train_loader,\n",
    "            per_timestep_val_loader=per_ts_val_loader,\n",
    "            sequence_val_loader=seq_val_loader,\n",
    "            config=config,\n",
    "            loss_config=selected_loss_config,\n",
    "            experiment_name=f'{MODEL_ARCH}_{LOSS_TYPE}'\n",
    "        )\n",
    "        print(f\"\\n\u2713 Trainer initialized with DUAL-BATCH mode\")\n",
    "\n",
    "    else:\n",
    "        # Sequence-only API\n",
    "        trainer = SimpleTrainer(\n",
    "            model=model_for_loss,\n",
    "            train_loader=seq_train_loader,\n",
    "            val_loader=seq_val_loader,\n",
    "            config=config,\n",
    "            loss_config=selected_loss_config,\n",
    "            experiment_name=f'{MODEL_ARCH}_{LOSS_TYPE}'\n",
    "        )\n",
    "        print(f\"\\n\u2713 Trainer initialized with SEQUENCE mode\")\n",
    "\n",
    "    print(f\"  Device: {trainer.device}\")\n",
    "    print(f\"  Optimizer: {type(trainer.optimizer).__name__}\")\n",
    "\n",
    "    # Check for weight optimizer (SA-BSP variants only)\n",
    "    if 'sa-bsp' in LOSS_TYPE:\n",
    "        if trainer.weight_optimizer is not None:\n",
    "            adapt_mode = trainer.adapt_mode\n",
    "            print(f\"  Weight optimizer: \u2713 Created for SA-BSP ({adapt_mode} mode)\")\n",
    "        else:\n",
    "            print(f\"  \u26a0 WARNING: SA-BSP but no weight_optimizer!\")\n",
    "\n",
    "    print(f\"\\n\ud83d\ude80 Starting training...\\n\")\n",
    "\n",
    "    # Train\n",
    "    results = trainer.train()\n",
    "\n",
    "    # Store results\n",
    "    key = f\"{MODEL_ARCH}_{LOSS_TYPE}\"\n",
    "    all_training_results[key] = results\n",
    "    all_trainers[key] = trainer\n",
    "    trained_models[key] = model_for_loss\n",
    "\n",
    "    print(f\"\\n\u2705 {LOSS_TYPE.upper()} training complete!\")\n",
    "    print(f\"   Best val loss: {results['best_val_loss']:.6f}\")\n",
    "    print(f\"   Final val loss: {results['val_history'][-1]['loss']:.6f}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ALL TRAINING COMPLETE!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Trained {len(all_training_results)} models with different loss functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Multi-Loss Training Comparison\n",
    "\n",
    "Compare training metrics across all 6 loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-loss comparison plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Color scheme for loss types\n",
    "colors = {\n",
    "    'baseline': '#1f77b4',       # Blue\n",
    "    'bsp': '#ff7f0e',             # Orange\n",
    "    'log-bsp': '#2ca02c',         # Green\n",
    "    'sa-bsp-perbin': '#d62728',   # Red\n",
    "    'sa-bsp-global': '#9467bd',   # Purple\n",
    "    'sa-bsp-combined': '#17becf'  # Cyan\n",
    "}\n",
    "linestyles = {\n",
    "    'baseline': '-', \n",
    "    'bsp': '--', \n",
    "    'log-bsp': '-.', \n",
    "    'sa-bsp-perbin': ':', \n",
    "    'sa-bsp-global': '-',\n",
    "    'sa-bsp-combined': '--'\n",
    "}\n",
    "markers = {\n",
    "    'baseline': 'o', \n",
    "    'bsp': 's', \n",
    "    'log-bsp': '^', \n",
    "    'sa-bsp-perbin': 'D', \n",
    "    'sa-bsp-global': 'v',\n",
    "    'sa-bsp-combined': 'p'\n",
    "}\n",
    "\n",
    "for loss_type in ['baseline', 'bsp', 'log-bsp', 'sa-bsp-perbin', 'sa-bsp-global', 'sa-bsp-combined']:\n",
    "    key = f\"{MODEL_ARCH}_{loss_type}\"\n",
    "    results = all_training_results[key]\n",
    "    \n",
    "    # Extract metrics\n",
    "    val_losses = [h['loss'] for h in results['val_history']]\n",
    "    val_mses = [h['mse'] for h in results['val_history']]\n",
    "    val_spectrum_errors = [h['spectrum_error'] for h in results['val_history']]\n",
    "    epochs = range(1, len(val_losses) + 1)\n",
    "\n",
    "    # Create label with short name\n",
    "    label_map = {\n",
    "        'baseline': 'BASELINE',\n",
    "        'bsp': 'BSP',\n",
    "        'log-bsp': 'Log-BSP',\n",
    "        'sa-bsp-perbin': 'SA-BSP (Per-bin)',\n",
    "        'sa-bsp-global': 'SA-BSP (Global)',\n",
    "        'sa-bsp-combined': 'SA-BSP (Combined)'\n",
    "    }\n",
    "    label = label_map[loss_type]\n",
    "\n",
    "    # Plot on all 3 axes\n",
    "    axes[0].plot(epochs, val_losses, label=label,\n",
    "                color=colors[loss_type], linestyle=linestyles[loss_type],\n",
    "                linewidth=2, alpha=0.9, marker=markers[loss_type], markersize=4, markevery=5)\n",
    "\n",
    "    axes[1].plot(epochs, val_mses, label=label,\n",
    "                color=colors[loss_type], linestyle=linestyles[loss_type],\n",
    "                linewidth=2, alpha=0.9, marker=markers[loss_type], markersize=4, markevery=5)\n",
    "    \n",
    "    axes[2].plot(epochs, val_spectrum_errors, label=label,\n",
    "                color=colors[loss_type], linestyle=linestyles[loss_type],\n",
    "                linewidth=2, alpha=0.9, marker=markers[loss_type], markersize=4, markevery=5)\n",
    "\n",
    "# Configure axes with LOG SCALE on y-axis\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Validation Loss', fontsize=12)\n",
    "axes[0].set_title('Validation Loss Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_yscale('log')  # LOG SCALE\n",
    "axes[0].set_ylim(bottom=1e-5, top=1.0)  # Clip for readability\n",
    "axes[0].legend(fontsize=9, loc='best')\n",
    "axes[0].grid(True, alpha=0.3, which='both')\n",
    "\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('MSE', fontsize=12)\n",
    "axes[1].set_title('MSE (Real Space)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_yscale('log')  # LOG SCALE\n",
    "axes[1].set_ylim(bottom=1e-5, top=1.0)  # Clip for readability\n",
    "axes[1].legend(fontsize=9, loc='best')\n",
    "axes[1].grid(True, alpha=0.3, which='both')\n",
    "\n",
    "axes[2].set_xlabel('Epoch', fontsize=12)\n",
    "axes[2].set_ylabel('Spectrum Error', fontsize=12)\n",
    "axes[2].set_title('Spectrum Error (Frequency Space)', fontsize=14, fontweight='bold')\n",
    "axes[2].set_yscale('log')  # LOG SCALE\n",
    "axes[2].set_ylim(bottom=1e-5, top=1.0)  # Clip for readability\n",
    "axes[2].legend(fontsize=9, loc='best')\n",
    "axes[2].grid(True, alpha=0.3, which='both')\n",
    "\n",
    "plt.suptitle(f'{MODEL_ARCH.upper()}: Loss Function Comparison (6 Variants)', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics table\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Final Metrics Summary\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"{'Loss Type':<25} {'Val Loss':<12} {'MSE':<15} {'Spectrum Error':<15}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for loss_type in ['baseline', 'bsp', 'log-bsp', 'sa-bsp-perbin', 'sa-bsp-global', 'sa-bsp-combined']:\n",
    "    key = f\"{MODEL_ARCH}_{loss_type}\"\n",
    "    results = all_training_results[key]\n",
    "    final_val = results['val_history'][-1]\n",
    "\n",
    "    label = loss_type.upper()\n",
    "    print(f\"{label:<25} {final_val['loss']:<12.6f} \"\n",
    "          f\"{final_val['mse']:<15.6f} {final_val['spectrum_error']:<15.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Spectral Bias Visualization (Energy Spectrum)\n",
    "\n",
    "Visualize E(k) vs wavenumber to identify spectral bias in trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.fft as fft\n",
    "from src.core.visualization.spectral_analysis import compute_unbinned_spectrum, compute_cached_true_spectrum\n",
    "from configs.visualization_config import SPECTRUM_CACHE_FILENAME, CACHE_DIR\n",
    "\n",
    "# Get validation batch for energy spectrum analysis\n",
    "print(\"Computing energy spectra for all trained models...\")\n",
    "\n",
    "# Check if models have been trained\n",
    "if 'trained_models' not in globals() or len(trained_models) == 0:\n",
    "    print(\"\\n\u26a0\ufe0f  WARNING: No trained models found!\")\n",
    "    print(\"   Please run Cell 12 (training) first before running this cell.\")\n",
    "    print(\"   This cell requires the 'trained_models' dictionary to be populated.\\n\")\n",
    "else:\n",
    "    print(f\"\u2713 Found {len(trained_models)} trained models\")\n",
    "    print(f\"  Keys: {list(trained_models.keys())}\\n\")\n",
    "\n",
    "val_batch_input, val_batch_target = next(iter(val_loader))\n",
    "\n",
    "# Move to device for inference\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "val_batch_input = val_batch_input.to(device)\n",
    "val_batch_target = val_batch_target.to(device)\n",
    "\n",
    "# Compute ground truth spectrum with percentile-based uncertainty bands from cache\n",
    "cache_path = f'{CACHE_DIR}/{SPECTRUM_CACHE_FILENAME}'\n",
    "print(f\"Loading true spectrum from cache: {cache_path}\")\n",
    "cached = np.load(cache_path)\n",
    "k_true = cached['unbinned_frequencies']  # Full FFT resolution (~2000 frequencies)\n",
    "E_true_median = cached['unbinned_energy_median']  # Median (50th percentile)\n",
    "E_true_p16 = cached['unbinned_energy_p16']        # Lower bound (16th percentile \u2248 -1\u03c3)\n",
    "E_true_p84 = cached['unbinned_energy_p84']        # Upper bound (84th percentile \u2248 +1\u03c3)\n",
    "print(f\"\u2713 True spectrum loaded ({len(k_true)} frequencies, unbinned)\")\n",
    "print(f\"  Using percentile-based uncertainty bands (16th-84th \u2248 \u00b11\u03c3)\")\n",
    "\n",
    "# Collect ALL validation predictions for uncertainty bands\n",
    "print(\"\\nComputing unbinned spectra with percentile-based uncertainty bands for all models...\")\n",
    "spectra = {}\n",
    "\n",
    "# Store true spectrum with percentile uncertainty bounds\n",
    "spectra['True'] = {\n",
    "    'frequencies': k_true,\n",
    "    'energy_median': E_true_median,\n",
    "    'energy_p16': E_true_p16,\n",
    "    'energy_p84': E_true_p84\n",
    "}\n",
    "\n",
    "for loss_type in ['baseline', 'bsp', 'log-bsp', 'sa-bsp-perbin', 'sa-bsp-global', 'sa-bsp-combined']:\n",
    "    key = f\"{MODEL_ARCH}_{loss_type}\"\n",
    "    \n",
    "    # Check if model exists\n",
    "    if key not in trained_models:\n",
    "        print(f\"  \u26a0\ufe0f  Skipping {loss_type.upper()}: model key '{key}' not found in trained_models\")\n",
    "        continue\n",
    "    \n",
    "    model_trained = trained_models[key]\n",
    "    model_trained.eval()\n",
    "    model_trained.to(device)\n",
    "    \n",
    "    try:\n",
    "        # Collect predictions from ALL validation batches for uncertainty bands\n",
    "        all_preds = []\n",
    "        print(f\"  Processing {loss_type.upper()}...\", end='')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for val_input, _ in val_loader:\n",
    "                val_input = val_input.to(device)\n",
    "                pred = model_trained(val_input)\n",
    "                all_preds.append(pred.cpu())\n",
    "        \n",
    "        # Stack all predictions: [total_val_samples, C, T]\n",
    "        all_preds_tensor = torch.cat(all_preds, dim=0)\n",
    "        \n",
    "        # Compute unbinned spectrum with percentile-based uncertainty bands\n",
    "        k_pred, E_pred_median, E_pred_p16, E_pred_p84 = compute_unbinned_spectrum(all_preds_tensor)\n",
    "        \n",
    "        # Create display label\n",
    "        label_map = {\n",
    "            'baseline': 'BASELINE',\n",
    "            'bsp': 'BSP',\n",
    "            'log-bsp': 'Log-BSP',\n",
    "            'sa-bsp-perbin': 'SA-BSP (Per-bin)',\n",
    "            'sa-bsp-global': 'SA-BSP (Global)',\n",
    "            'sa-bsp-combined': 'SA-BSP (Combined)'\n",
    "        }\n",
    "        spec_key = f\"{MODEL_ARCH.upper()} + {label_map[loss_type]}\"\n",
    "        \n",
    "        spectra[spec_key] = {\n",
    "            'frequencies': k_pred,\n",
    "            'energy_median': E_pred_median,\n",
    "            'energy_p16': E_pred_p16,\n",
    "            'energy_p84': E_pred_p84\n",
    "        }\n",
    "        \n",
    "        print(f\" \u2713 ({all_preds_tensor.shape[0]} samples)\")\n",
    "    except Exception as e:\n",
    "        print(f\" \u274c Error: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n\u2713 Spectra computed for {len(spectra)} entries with percentile-based uncertainty bands\\n\")\n",
    "\n",
    "# Plot energy spectrum with percentile-based uncertainty bands (safe for log scale!)\n",
    "fig, ax = plt.subplots(figsize=(14, 9))\n",
    "\n",
    "# Color scheme for loss types\n",
    "colors_plot = {\n",
    "    'True': '#000000',  # Black for ground truth\n",
    "    'baseline': '#1f77b4',\n",
    "    'bsp': '#ff7f0e',\n",
    "    'log-bsp': '#2ca02c',\n",
    "    'sa-bsp-perbin': '#d62728',\n",
    "    'sa-bsp-global': '#9467bd',\n",
    "    'sa-bsp-combined': '#17becf'\n",
    "}\n",
    "\n",
    "# Plot ground truth with uncertainty band (black)\n",
    "if 'True' in spectra:\n",
    "    data = spectra['True']\n",
    "    k = data['frequencies']\n",
    "    E_median = data['energy_median']\n",
    "    E_p16 = data['energy_p16']\n",
    "    E_p84 = data['energy_p84']\n",
    "    \n",
    "    # Plot median line\n",
    "    ax.loglog(k, E_median, color=colors_plot['True'], linewidth=3, \n",
    "             label='True (Real Data)', zorder=10, alpha=0.9)\n",
    "    \n",
    "    # Plot percentile-based uncertainty band (16th-84th percentiles \u2248 \u00b11\u03c3)\n",
    "    # These are GUARANTEED to be positive \u2192 safe for log scale!\n",
    "    ax.fill_between(k, E_p16, E_p84,\n",
    "                     color=colors_plot['True'], alpha=0.15, zorder=9,\n",
    "                     label='True (16th-84th percentile)')\n",
    "\n",
    "# Plot model predictions with percentile-based uncertainty bands\n",
    "for loss_type in ['baseline', 'bsp', 'log-bsp', 'sa-bsp-perbin', 'sa-bsp-global', 'sa-bsp-combined']:\n",
    "    label_map = {\n",
    "        'baseline': 'BASELINE',\n",
    "        'bsp': 'BSP',\n",
    "        'log-bsp': 'Log-BSP',\n",
    "        'sa-bsp-perbin': 'SA-BSP (Per-bin)',\n",
    "        'sa-bsp-global': 'SA-BSP (Global)',\n",
    "        'sa-bsp-combined': 'SA-BSP (Combined)'\n",
    "    }\n",
    "    label_key = f\"{MODEL_ARCH.upper()} + {label_map[loss_type]}\"\n",
    "    \n",
    "    # Check if spectrum exists before plotting\n",
    "    if label_key not in spectra:\n",
    "        print(f\"  \u26a0\ufe0f  Skipping plot for {loss_type.upper()}: '{label_key}' not in spectra dictionary\")\n",
    "        continue\n",
    "    \n",
    "    data = spectra[label_key]\n",
    "    k = data['frequencies']\n",
    "    E_median = data['energy_median']\n",
    "    E_p16 = data['energy_p16']\n",
    "    E_p84 = data['energy_p84']\n",
    "    \n",
    "    color = colors_plot[loss_type]\n",
    "    \n",
    "    # Plot median line\n",
    "    ax.loglog(k, E_median, color=color, linewidth=2.5, \n",
    "             alpha=0.85, label=label_key, zorder=5)\n",
    "    \n",
    "    # Plot percentile-based uncertainty band (guaranteed positive for log scale)\n",
    "    ax.fill_between(k, E_p16, E_p84,\n",
    "                     color=color, alpha=0.12, zorder=4)\n",
    "\n",
    "# Configure plot\n",
    "ax.set_xlabel('Frequency (normalized)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('E(k) - Spectral Power', fontsize=14, fontweight='bold')\n",
    "ax.set_title(f'Energy Spectrum Comparison with Percentile Uncertainty Bands\\n{MODEL_ARCH.upper()} Model (6 Loss Variants)', \n",
    "            fontsize=16, fontweight='bold')\n",
    "ax.legend(fontsize=10, loc='best', framealpha=0.95, ncol=1)\n",
    "ax.grid(True, alpha=0.3, which='both', linestyle='--')\n",
    "\n",
    "# Set frequency range to 0-25Hz (full dataset range)\n",
    "ax.set_xlim(0, 25.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\u2713 Energy spectrum plot complete\")\n",
    "print(f\"  \u2022 Unbinned spectrum: Full FFT resolution (~{len(k_true)} frequencies)\")\n",
    "print(f\"  \u2022 Uncertainty bands: 16th-84th percentiles (\u2248 \u00b11\u03c3) across all validation samples\")\n",
    "print(f\"  \u2022 Percentiles are ALWAYS positive \u2192 safe for log-scale display!\")\n",
    "print(f\"  \u2022 This visualization shows spectral bias: deviation from ground truth at high frequencies\")\n",
    "print(f\"  \u2022 Log-BSP and SA-BSP variants should show better high-frequency matching than baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Spectral Bias Quantification\n",
    "\n",
    "Compute spectral bias metrics to quantify how well each model captures high-frequency content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core.visualization.spectral_analysis import compute_spectral_bias_metric\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SPECTRAL BIAS METRICS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nQuantifies how well each model captures different frequency ranges.\")\n",
    "print(\"Spectral Bias Ratio = High Freq Error / Low Freq Error\")\n",
    "print(\"  - Ratio > 2.0: Significant spectral bias (struggles with high frequencies)\")\n",
    "print(\"  - Ratio > 1.5: Moderate spectral bias\")\n",
    "print(\"  - Ratio \u2264 1.5: Low spectral bias (captures frequencies well)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compute metrics for each trained model\n",
    "spectral_metrics = {}\n",
    "\n",
    "for loss_type in ['baseline', 'bsp', 'log-bsp', 'sa-bsp-perbin', 'sa-bsp-global', 'sa-bsp-combined']:\n",
    "    key = f\"{MODEL_ARCH}_{loss_type}\"\n",
    "    model_trained = trained_models[key]\n",
    "    model_trained.eval()\n",
    "    model_trained.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = model_trained(val_input)\n",
    "    \n",
    "    metrics = compute_spectral_bias_metric(pred.cpu(), val_target.cpu(), n_bins=32)\n",
    "    spectral_metrics[loss_type] = metrics\n",
    "    \n",
    "    label_map = {\n",
    "        'baseline': 'BASELINE',\n",
    "        'bsp': 'BSP',\n",
    "        'log-bsp': 'Log-BSP',\n",
    "        'sa-bsp-perbin': 'SA-BSP (Per-bin)',\n",
    "        'sa-bsp-global': 'SA-BSP (Global)',\n",
    "        'sa-bsp-combined': 'SA-BSP (Combined)'\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{MODEL_ARCH.upper()} + {label_map[loss_type]}:\")\n",
    "    print(f\"  Low frequency error:   {metrics['low_freq_error']:.6f}\")\n",
    "    print(f\"  Mid frequency error:   {metrics['mid_freq_error']:.6f}\")\n",
    "    print(f\"  High frequency error:  {metrics['high_freq_error']:.6f}\")\n",
    "    print(f\"  Spectral bias ratio:   {metrics['spectral_bias_ratio']:.4f}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    if metrics['spectral_bias_ratio'] > 2.0:\n",
    "        print(f\"  \u2192 \u26a0\ufe0f  SIGNIFICANT spectral bias detected!\")\n",
    "        print(f\"     Model struggles with high-frequency content\")\n",
    "    elif metrics['spectral_bias_ratio'] > 1.5:\n",
    "        print(f\"  \u2192 \u26a1 MODERATE spectral bias\")\n",
    "        print(f\"     Some difficulty with high frequencies\")\n",
    "    else:\n",
    "        print(f\"  \u2192 \u2705 LOW spectral bias\")\n",
    "        print(f\"     Model captures frequency content well\")\n",
    "\n",
    "# Create comparison visualization\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Spectral Bias Comparison\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Bar plot 1: Frequency errors\n",
    "loss_types = ['baseline', 'bsp', 'log-bsp', 'sa-bsp-perbin', 'sa-bsp-global', 'sa-bsp-combined']\n",
    "x = np.arange(len(loss_types))\n",
    "width = 0.2\n",
    "\n",
    "low_errors = [spectral_metrics[lt]['low_freq_error'] for lt in loss_types]\n",
    "mid_errors = [spectral_metrics[lt]['mid_freq_error'] for lt in loss_types]\n",
    "high_errors = [spectral_metrics[lt]['high_freq_error'] for lt in loss_types]\n",
    "\n",
    "ax1.bar(x - width, low_errors, width, label='Low Freq', color='#2ca02c', alpha=0.8)\n",
    "ax1.bar(x, mid_errors, width, label='Mid Freq', color='#ff7f0e', alpha=0.8)\n",
    "ax1.bar(x + width, high_errors, width, label='High Freq', color='#d62728', alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('Loss Type', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Frequency Error', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Frequency Range Errors', fontsize=14, fontweight='bold')\n",
    "ax1.set_yscale('log')  # LOG SCALE\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(['BASE', 'BSP', 'Log-BSP', 'SA-Per', 'SA-Glob', 'SA-Comb'], rotation=15, ha='right')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3, axis='y', which='both')\n",
    "\n",
    "# Bar plot 2: Spectral bias ratio\n",
    "bias_ratios = [spectral_metrics[lt]['spectral_bias_ratio'] for lt in loss_types]\n",
    "colors_bars = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#17becf']\n",
    "\n",
    "bars = ax2.bar(x, bias_ratios, color=colors_bars, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Add threshold lines\n",
    "ax2.axhline(y=2.0, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Significant bias threshold')\n",
    "ax2.axhline(y=1.5, color='orange', linestyle='--', linewidth=2, alpha=0.7, label='Moderate bias threshold')\n",
    "\n",
    "ax2.set_xlabel('Loss Type', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Spectral Bias Ratio', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Spectral Bias Ratio (High/Low)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(['BASE', 'BSP', 'Log-BSP', 'SA-Per', 'SA-Glob', 'SA-Comb'], rotation=15, ha='right')\n",
    "ax2.legend(fontsize=10, loc='upper right')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, ratio) in enumerate(zip(bars, bias_ratios)):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "            f'{ratio:.2f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.suptitle(f'{MODEL_ARCH.upper()}: Spectral Bias Analysis (6 Loss Variants)', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"\u2705 Spectral bias analysis complete!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(\"\\nKey Findings:\")\n",
    "print(\"  \u2022 Baseline: Pure MSE - typically shows significant spectral bias\")\n",
    "print(\"  \u2022 BSP: Fixed spectral loss with k\u00b2 weighting - moderate improvement\")\n",
    "print(\"  \u2022 Log-BSP: Log-domain spectral loss - addresses wide dynamic range\")\n",
    "print(\"  \u2022 SA-BSP (Per-bin): Adaptive per-bin weights - emphasize hard frequencies\")\n",
    "print(\"  \u2022 SA-BSP (Global): Adaptive MSE/BSP balance - optimize overall trade-off\")\n",
    "print(\"  \u2022 SA-BSP (Combined): Full competitive dynamics - most expressive approach\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. \u2713 Loading real CDON data with proper normalization\n",
    "2. \u2713 Creating neural operator models (DeepONet, FNO, UNet)\n",
    "3. \u2713 **Sequential training with all 6 loss functions**:\n",
    "   - **BASELINE**: Relative L2 loss only (MSE baseline)\n",
    "   - **BSP**: MSE + fixed BSP loss with k\u00b2 weighting\n",
    "   - **Log-BSP**: MSE + BSP with log\u2081\u2080 spectral energies (uniform weighting)\n",
    "   - **SA-BSP (Per-bin)**: MSE + 32 adaptive per-bin weights (negated gradients for frequency emphasis)\n",
    "   - **SA-BSP (Global)**: MSE + 2 adaptive weights (w_mse + w_bsp, negated gradients for MSE/BSP balance)\n",
    "   - **SA-BSP (Combined)**: MSE + 34 weights (w_mse + w_bsp + 32 per-bin, all negated gradients for full competitive dynamics)\n",
    "4. \u2713 **Multi-loss comparison plots** showing training metrics\n",
    "5. \u2713 **Energy spectrum visualization** (E(k) vs wavenumber) to identify spectral bias\n",
    "6. \u2713 **Spectral bias quantification** with metrics and comparison plots\n",
    "\n",
    "**Key Results:**\n",
    "- All 6 loss types trained on the same model architecture\n",
    "- Direct comparison shows which loss function best mitigates spectral bias\n",
    "- Energy spectrum plot reveals how well each model captures high-frequency content\n",
    "- Quantitative metrics identify spectral bias ratio for each approach\n",
    "\n",
    "**SA-PINNs Implementation:**\n",
    "- **Per-bin mode**: Uses negated gradients (ascent) to emphasize difficult frequency bins\n",
    "- **Global mode**: Uses negated gradients (ascent) to learn optimal MSE/BSP balance via competitive dynamics\n",
    "- **Combined mode**: Full competitive dynamics with all weights (w_mse, w_bsp, and 32 per-bin) using negated gradients\n",
    "\n",
    "**Experiment with different configurations:**\n",
    "- **Cell 0**: Run to force reload modules after code changes\n",
    "- **Cell 3**: Change `MODEL_ARCH` to try different models ('deeponet', 'fno', 'unet')\n",
    "- **Cell 5**: Adjust hyperparameters (epochs, learning rate, etc.) in TrainingConfig\n",
    "- Run all cells sequentially to train and compare all 6 loss types automatically!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}