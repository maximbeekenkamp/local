{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Operator Training Demo: CDON Dataset\n",
    "\n",
    "This notebook demonstrates end-to-end training of neural operator models (DeepONet, FNO, UNet) on the CDON dataset.\n",
    "\n",
    "**Features:**\n",
    "- Trains on **real CDON data**\n",
    "- **Configurable loss functions** (Baseline, BSP, SA-BSP)\n",
    "- Minimal custom code - reuses existing codebase\n",
    "- Includes visualizations of training progress\n",
    "- Compatible with Google Colab\n",
    "\n",
    "**Models available:**\n",
    "- `deeponet`: Branch-trunk architecture (~235K params)\n",
    "- `fno`: Fourier Neural Operator (~261K params)\n",
    "- `unet`: Encoder-decoder with skip connections (~249K params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup & Imports (Colab-Ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab setup\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure we're in /content\n",
    "try:\n",
    "    os.chdir('/content')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Clone repository if running in Colab\n",
    "repo_path = Path('/content/local')\n",
    "if not repo_path.exists():\n",
    "    print(\"ðŸ“¥ Cloning repository...\")\n",
    "    !git clone https://github.com/maximbeekenkamp/local.git\n",
    "    print(\"âœ… Repository cloned\")\n",
    "else:\n",
    "    print(\"âœ… Repository exists\")\n",
    "\n",
    "# Change to repo directory\n",
    "try:\n",
    "    os.chdir('/content/local')\n",
    "    print(f\"âœ… Changed to: {os.getcwd()}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Install dependencies\n",
    "print(\"\\nðŸ“¦ Installing dependencies...\")\n",
    "!pip install -r requirements.txt -q\n",
    "print(\"âœ… Dependencies installed\")\n",
    "\n",
    "# Standard imports\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Project imports\n",
    "from src.core.data_processing.cdon_dataset import CDONDataset\n",
    "from src.core.data_processing.cdon_transforms import CDONNormalization\n",
    "from src.core.models.model_factory import create_model\n",
    "from src.core.training.simple_trainer import SimpleTrainer\n",
    "from configs.training_config import TrainingConfig\n",
    "\n",
    "print(\"\\nâœ“ Imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Load Real CDON Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get project root\n",
    "project_root = Path.cwd()\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Data directory\n",
    "DATA_DIR = project_root / 'CDONData'\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "\n",
    "# Create normalization object (required by CDONDataset)\n",
    "stats_path = project_root / 'configs' / 'cdon_stats.json'\n",
    "print(f\"Loading stats from: {stats_path}\")\n",
    "normalizer = CDONNormalization(stats_path=str(stats_path))\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CDONDataset(\n",
    "    data_dir=str(DATA_DIR),\n",
    "    split='train',\n",
    "    normalize=normalizer  # Pass normalizer object, not boolean\n",
    ")\n",
    "\n",
    "val_dataset = CDONDataset(\n",
    "    data_dir=str(DATA_DIR),\n",
    "    split='test',\n",
    "    normalize=normalizer\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Data loaded successfully\")\n",
    "print(f\"  Train samples: {len(train_dataset)}\")\n",
    "print(f\"  Val samples: {len(val_dataset)}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# Inspect a sample\n",
    "sample_input, sample_target = train_dataset[0]\n",
    "print(f\"\\nSample shapes:\")\n",
    "print(f\"  Input: {sample_input.shape}\")\n",
    "print(f\"  Target: {sample_target.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Choose Model Architecture\n",
    "\n",
    "**Change `MODEL_ARCH` to try different models:**\n",
    "- `'deeponet'`: Branch-trunk architecture\n",
    "- `'fno'`: Fourier Neural Operator\n",
    "- `'unet'`: U-Net encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model architecture\n",
    "MODEL_ARCH = 'deeponet'  # Options: 'deeponet', 'fno', 'unet'\n",
    "\n",
    "model = create_model(MODEL_ARCH)\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"âœ“ Created {MODEL_ARCH.upper()} model\")\n",
    "print(f\"  Parameters: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4A: Import Loss Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.loss_config import BASELINE_CONFIG, BSP_CONFIG, SA_BSP_CONFIG\n",
    "from src.core.evaluation.loss_factory import create_loss\n",
    "\n",
    "print(\"âœ“ Loss configurations imported successfully\")\n",
    "print(\"\\nAvailable loss types:\")\n",
    "print(f\"  1. BASELINE: {BASELINE_CONFIG.description}\")\n",
    "print(f\"  2. BSP:      {BSP_CONFIG.description}\")\n",
    "print(f\"  3. SA-BSP:   {SA_BSP_CONFIG.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4B: Select Loss Type\n",
    "\n",
    "**Change `LOSS_TYPE` below to experiment:**\n",
    "- `'baseline'`: Standard Relative L2 loss (default)\n",
    "- `'bsp'`: Binned Spectral Power loss\n",
    "- `'sa-bsp'`: Self-Adaptive BSP with learnable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose loss type (CHANGE THIS TO EXPERIMENT)\n",
    "LOSS_TYPE = 'baseline'  # Options: 'baseline', 'bsp', 'sa-bsp'\n",
    "\n",
    "# Map to configuration\n",
    "loss_config_map = {\n",
    "    'baseline': BASELINE_CONFIG,\n",
    "    'bsp': BSP_CONFIG,\n",
    "    'sa-bsp': SA_BSP_CONFIG\n",
    "}\n",
    "\n",
    "if LOSS_TYPE not in loss_config_map:\n",
    "    raise ValueError(f\"Invalid LOSS_TYPE: '{LOSS_TYPE}'\")\n",
    "\n",
    "selected_loss_config = loss_config_map[LOSS_TYPE]\n",
    "\n",
    "print(f\"âœ“ Selected loss type: {LOSS_TYPE.upper()}\")\n",
    "print(f\"  Description: {selected_loss_config.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4C: Create and Validate Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss function\n",
    "criterion = create_loss(selected_loss_config)\n",
    "\n",
    "print(f\"âœ“ Loss function created: {type(criterion).__name__}\")\n",
    "\n",
    "# Validate with dummy tensors\n",
    "dummy_pred = torch.randn(4, 1, 1000)\n",
    "dummy_target = torch.randn(4, 1, 1000)\n",
    "\n",
    "test_loss = criterion(dummy_pred, dummy_target)\n",
    "print(f\"âœ“ Dummy loss value: {test_loss.item():.6f}\")\n",
    "print(f\"âœ“ Loss is finite: {torch.isfinite(test_loss).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4D: Test Loss on Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on real CDON data\n",
    "sample_batch_input, sample_batch_target = next(iter(train_loader))\n",
    "\n",
    "real_data_loss = criterion(sample_batch_input, sample_batch_target)\n",
    "\n",
    "print(f\"âœ“ Loss on real data: {real_data_loss.item():.6f}\")\n",
    "print(f\"âœ“ Loss is finite: {torch.isfinite(real_data_loss).item()}\")\n",
    "print(f\"âœ“ Ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TrainingConfig(\n",
    "    num_epochs=50,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    weight_decay=1e-4,\n",
    "    scheduler_type='cosine',\n",
    "    cosine_eta_min=1e-6,\n",
    "    eval_metrics=['field_error', 'spectrum_error'],\n",
    "    eval_frequency=1,\n",
    "    checkpoint_dir=f'checkpoints/{MODEL_ARCH}',\n",
    "    save_best=False,      # Disabled for debugging\n",
    "    save_latest=False,    # Disabled for debugging\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    num_workers=2,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Training configuration:\")\n",
    "print(f\"  Epochs: {config.num_epochs}\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")\n",
    "print(f\"  Device: {config.device}\")\n",
    "print(f\"  âš  Checkpointing: DISABLED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Create Trainer and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer with loss_config (required parameter)\n",
    "trainer = SimpleTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    config=config,\n",
    "    loss_config=selected_loss_config,  # Required parameter\n",
    "    experiment_name=f'{MODEL_ARCH}_{LOSS_TYPE}'\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Trainer initialized\")\n",
    "print(f\"  Device: {trainer.device}\")\n",
    "print(f\"  Optimizer: {type(trainer.optimizer).__name__}\")\n",
    "print(f\"  Loss: {type(trainer.criterion).__name__}\")\n",
    "\n",
    "# Check for weight optimizer (SA-BSP only)\n",
    "if LOSS_TYPE == 'sa-bsp':\n",
    "    if trainer.weight_optimizer is not None:\n",
    "        print(f\"  Weight optimizer: {type(trainer.weight_optimizer).__name__} âœ“\")\n",
    "    else:\n",
    "        print(f\"  âš  WARNING: SA-BSP selected but weight_optimizer is None!\")\n",
    "else:\n",
    "    print(f\"  Weight optimizer: None\")\n",
    "\n",
    "print(f\"\\nStarting training...\\n\")\n",
    "\n",
    "# Train\n",
    "results = trainer.train()\n",
    "\n",
    "print(f\"\\nâœ“ Training complete!\")\n",
    "print(f\"  Best val loss: {results['best_val_loss']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics\n",
    "train_losses = [h['loss'] for h in results['train_history']]\n",
    "val_losses = [h['loss'] for h in results['val_history']]\n",
    "val_field_errors = [h['field_error'] for h in results['val_history']]\n",
    "val_spectrum_errors = [h['spectrum_error'] for h in results['val_history']]\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "# Create plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: Loss\n",
    "axes[0].plot(epochs, train_losses, label='Train Loss', marker='o', markersize=3)\n",
    "axes[0].plot(epochs, val_losses, label='Val Loss', marker='s', markersize=3)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title(f'Training Loss ({LOSS_TYPE.upper()})')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Field Error\n",
    "axes[1].plot(epochs, val_field_errors, label='Val Field Error', marker='s', markersize=3, color='orange')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Field Error')\n",
    "axes[1].set_title('Field Error (Real Space)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Spectrum Error\n",
    "axes[2].plot(epochs, val_spectrum_errors, label='Val Spectrum Error', marker='s', markersize=3, color='green')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Spectrum Error')\n",
    "axes[2].set_title('Spectrum Error (Frequency Space)')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'{MODEL_ARCH.upper()} Training Results', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "print(f\"\\nFinal Metrics:\")\n",
    "print(f\"  Train Loss: {train_losses[-1]:.6f}\")\n",
    "print(f\"  Val Loss: {val_losses[-1]:.6f}\")\n",
    "print(f\"  Val Field Error: {val_field_errors[-1]:.6f}\")\n",
    "print(f\"  Val Spectrum Error: {val_spectrum_errors[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. âœ“ Loading real CDON data with proper normalization\n",
    "2. âœ“ Creating neural operator models (DeepONet, FNO, UNet)\n",
    "3. âœ“ **Configurable loss functions** (Baseline, BSP, SA-BSP)\n",
    "4. âœ“ Training with SimpleTrainer\n",
    "5. âœ“ Visualizing training metrics\n",
    "\n",
    "**Experiment with different configurations:**\n",
    "- **Cell 3**: Change `MODEL_ARCH` to try different models\n",
    "- **Cell 4B**: Change `LOSS_TYPE` to try different loss functions\n",
    "- **Cell 5**: Adjust hyperparameters (epochs, learning rate, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
