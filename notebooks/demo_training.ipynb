{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Operator Training Demo: CDON Dataset\n",
    "\n",
    "This notebook demonstrates end-to-end training of neural operator models (DeepONet, FNO, UNet) on the CDON dataset.\n",
    "\n",
    "**Features:**\n",
    "- Trains on **real CDON data**\n",
    "- **Sequential training with all 6 loss functions**:\n",
    "  - **BASELINE**: Relative L2 loss only (baseline MSE)\n",
    "  - **BSP**: MSE + fixed BSP loss with k\u00b2 weighting\n",
    "  - **Log-BSP**: MSE + BSP with log\u2081\u2080 spectral energies (uniform \u03bb_k weighting)\n",
    "  - **SA-BSP (Per-bin)**: MSE + 32 adaptive per-bin weights (negated gradients for frequency emphasis)\n",
    "  - **SA-BSP (Global)**: MSE + 2 adaptive weights (w_mse + w_bsp) for MSE/BSP balance\n",
    "  - **SA-BSP (Combined)**: MSE + 34 weights (w_mse + w_bsp + 32 per-bin) with full competitive dynamics\n",
    "- **Multi-loss comparison plots** showing training metrics\n",
    "- **Energy spectrum visualization** (E(k) vs wavenumber) to identify spectral bias\n",
    "- **Spectral bias quantification** with metrics and comparison plots\n",
    "- Compatible with Google Colab\n",
    "\n",
    "**Models available:**\n",
    "- `deeponet`: Branch-trunk architecture with SIREN activation (~235K params)\n",
    "- `fno`: Fourier Neural Operator (~261K params)\n",
    "- `unet`: Encoder-decoder with skip connections (~249K params)\n",
    "\n",
    "**SA-PINNs Implementation:**\n",
    "Uses saddle-point optimization with negated gradients (gradient ascent on loss) to enable competitive dynamics. This automatically emphasizes difficult frequency bins and finds optimal loss balance through min-max optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 0: Force Reload Modules\n",
    "\n",
    "Run this cell to reload all project modules after code changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force reload of all modules\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Get list of all loaded modules from the project\n",
    "modules_to_reload = []\n",
    "for module_name in list(sys.modules.keys()):\n",
    "    if any(x in module_name for x in ['src.', 'configs.']):\n",
    "        modules_to_reload.append(module_name)\n",
    "\n",
    "# Remove modules from sys.modules to force reload\n",
    "for module_name in modules_to_reload:\n",
    "    if module_name in sys.modules:\n",
    "        del sys.modules[module_name]\n",
    "\n",
    "print(f\"\u2713 Cleared {len(modules_to_reload)} cached modules\")\n",
    "print(\"  Run Cell 1 to reimport all modules with latest code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup & Imports (Colab-Ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab setup\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure we're in /content\n",
    "try:\n",
    "    os.chdir('/content')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Clone repository if running in Colab\n",
    "repo_path = Path('/content/local')\n",
    "if not repo_path.exists():\n",
    "    print(\"\ud83d\udce5 Cloning repository...\")\n",
    "    !git clone https://github.com/maximbeekenkamp/local.git\n",
    "    print(\"\u2705 Repository cloned\")\n",
    "else:\n",
    "    print(\"\ud83d\udce5 Updating repository...\")\n",
    "    !git -C /content/local pull\n",
    "    print(\"\u2705 Repository updated\")\n",
    "\n",
    "# Change to repo directory\n",
    "try:\n",
    "    os.chdir('/content/local')\n",
    "    print(f\"\u2705 Changed to: {os.getcwd()}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Install dependencies - IMPORTANT: Upgrade numpy FIRST to avoid binary incompatibility\n",
    "print(\"\\n\ud83d\udce6 Installing dependencies...\")\n",
    "print(\"\ud83d\udd27 Upgrading numpy to 2.x (fixes binary compatibility)...\")\n",
    "!pip install \"numpy>=2.0.0\" --upgrade -q\n",
    "print(\"\u2705 NumPy upgraded\")\n",
    "\n",
    "print(\"\ud83d\udce6 Installing other dependencies...\")\n",
    "!pip install -r requirements.txt -q\n",
    "print(\"\u2705 Dependencies installed\")\n",
    "\n",
    "# Standard imports\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Project imports\n",
    "from src.core.data_processing.cdon_dataset import CDONDataset\n",
    "from src.core.data_processing.cdon_transforms import CDONNormalization\n",
    "from src.core.models.model_factory import create_model\n",
    "from src.core.training.simple_trainer import SimpleTrainer\n",
    "from configs.training_config import TrainingConfig\n",
    "\n",
    "print(\"\\n\u2713 Imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Configuration\n",
    "\n",
    "Configure optional features before loading data:\n",
    "- **Causal padding**: Zero-padding preprocessing (Reference CausalityDeepONet)\n",
    "- **DeepONet activation**: Choose activation function (REQU, TANH, RELU, SIREN)\n",
    "- **Penalty loss**: Optional inverse-variance weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# CONFIGURATION\n# ============================================================================\n\n# 1. CAUSALITY: Zero-padding preprocessing (Reference CausalityDeepONet)\nUSE_CAUSAL_PADDING = True  # ENABLED BY DEFAULT (matches reference)\n# Set to False to disable causal padding (standard preprocessing)\n\n# 2. DEEPONET ACTIVATION: Choose activation function\nDEEPONET_ACTIVATION = 'requ'  # Options: 'requ' (default), 'tanh', 'relu', 'siren'\n# 'requ' = ReLU\u00b2 (reference default, smooth gradients)\n# 'tanh' = Stable for operator learning\n# 'relu' = Standard ReLU\n# 'siren' = Sinusoidal activation (requires siren-pytorch)\n\n# 3. PENALTY LOSS: Optional inverse-variance weighting\nUSE_PENALTY_LOSS = False  # Set to True to enable penalty weighting\nPENALTY_EPSILON = 1e-8     # Numerical stability for penalty\nPENALTY_PER_SAMPLE = True  # Per-sample (True) or global (False) penalty\n\nprint(\"\u2713 Configuration loaded:\")\nprint(f\"  Causal padding:     {'ENABLED' if USE_CAUSAL_PADDING else 'DISABLED'}\")\nprint(f\"  DeepONet activation: {DEEPONET_ACTIVATION.upper()}\")\nprint(f\"  Penalty loss:       {'ENABLED' if USE_PENALTY_LOSS else 'DISABLED'}\")\nprint()\n\n# ============================================================================\n# LOAD REAL CDON DATA\n# ============================================================================\n\n# Get project root\nproject_root = Path.cwd()\nprint(f\"Project root: {project_root}\")\n\n# Data directory\nDATA_DIR = project_root / 'CDONData'\nprint(f\"Data directory: {DATA_DIR}\")\n\n# Create normalization object (required by CDONDataset)\nstats_path = project_root / 'configs' / 'cdon_stats.json'\nprint(f\"Loading stats from: {stats_path}\")\nnormalizer = CDONNormalization(stats_path=str(stats_path))\n\n# Create datasets with optional causal padding\ntrain_dataset = CDONDataset(\n    data_dir=str(DATA_DIR),\n    split='train',\n    normalize=normalizer,\n    mode='sequence',  # Use sequence mode for BSP loss training\n    use_causal_sequence=USE_CAUSAL_PADDING,  # Apply causal padding if enabled\n    signal_length=4000\n)\n\nval_dataset = CDONDataset(\n    data_dir=str(DATA_DIR),\n    split='test',\n    normalize=normalizer,\n    mode='sequence',  # Use sequence mode for BSP loss training\n    use_causal_sequence=USE_CAUSAL_PADDING,  # Apply causal padding if enabled\n    signal_length=4000\n)\n\n# Create dataloaders\nBATCH_SIZE = 2\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=2,\n    pin_memory=True\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True\n)\n\nprint(f\"\\n\u2713 Data loaded successfully\")\nprint(f\"  Train samples: {len(train_dataset)}\")\nprint(f\"  Val samples: {len(val_dataset)}\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\n\n# Inspect a sample\nsample_input, sample_target = train_dataset[0]\nprint(f\"\\nSample shapes:\")\nprint(f\"  Input: {sample_input.shape}\")\nprint(f\"  Target: {sample_target.shape}\")\n\nif USE_CAUSAL_PADDING:\n    expected_input_len = 4000 + (4000 - 1)  # signal_length + padding\n    if sample_input.shape[-1] == expected_input_len:\n        print(f\"  \u2713 Causal padding applied correctly (input length: {expected_input_len})\")\n    else:\n        print(f\"  \u26a0 Warning: Expected input length {expected_input_len}, got {sample_input.shape[-1]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Choose Model Architecture\n",
    "\n",
    "**Change `MODEL_ARCH` to try different models:**\n",
    "- `'deeponet'`: Branch-trunk architecture with SIREN activation\n",
    "- `'fno'`: Fourier Neural Operator\n",
    "- `'unet'`: U-Net encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model architecture\n",
    "MODEL_ARCH = 'deeponet'  # Options: 'deeponet', 'fno', 'unet'\n",
    "\n",
    "# Create model with optional DeepONet activation\n",
    "if MODEL_ARCH == 'deeponet':\n",
    "    model = create_model(MODEL_ARCH, config={'activation': DEEPONET_ACTIVATION})\n",
    "    print(f\"\u2713 Created {MODEL_ARCH.upper()} model with {DEEPONET_ACTIVATION.upper()} activation\")\n",
    "else:\n",
    "    model = create_model(MODEL_ARCH)\n",
    "    print(f\"\u2713 Created {MODEL_ARCH.upper()} model\")\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"  Parameters: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3.5: Loss Variant Configuration (Quick Switch)\n",
    "\n",
    "**Easily switch between loss variants by changing `LOSS_VARIANT`:**\n",
    "\n",
    "Available variants:\n",
    "- `'bsp'`: Fixed BSP with k\u00b2 weighting (static spectral loss)\n",
    "- `'log_bsp'`: Log-domain BSP with uniform weighting (wide dynamic range)\n",
    "- `'sa_bsp'`: Self-Adaptive BSP with trainable per-bin weights (emphasis on difficult frequencies)\n",
    "- `'sa_log_bsp'`: Self-Adaptive Log-BSP (combines adaptive weights with log-domain energies)\n",
    "\n",
    "Each variant can use different adaptation modes (per-bin, global, combined) via `SA_ADAPT_MODE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOSS VARIANT CONFIGURATION - Change this to switch between loss types\n",
    "# ============================================================================\n",
    "\n",
    "# Choose loss variant\n",
    "LOSS_VARIANT = 'bsp'  # Options: 'bsp', 'log_bsp', 'sa_bsp', 'sa_log_bsp'\n",
    "\n",
    "# For SA-BSP variants, choose adaptation mode\n",
    "SA_ADAPT_MODE = 'per-bin'  # Options: 'per-bin', 'global', 'combined'\n",
    "# 'per-bin': 32 trainable weights (one per frequency bin)\n",
    "# 'global': 2 trainable weights (w_mse + w_bsp for MSE/BSP balance)\n",
    "# 'combined': 34 trainable weights (w_mse + w_bsp + 32 per-bin)\n",
    "\n",
    "# Common parameters for all loss variants\n",
    "N_BINS = 32\n",
    "SIGNAL_LENGTH = 4000\n",
    "CACHE_PATH = 'cache/true_spectrum.npz'\n",
    "EPSILON = 1e-8\n",
    "\n",
    "# Loss variant configurations\n",
    "LOSS_CONFIGS = {\n",
    "    # Fixed BSP with k\u00b2 weighting\n",
    "    'bsp': {\n",
    "        'loss_type': 'combined',\n",
    "        'loss_params': {\n",
    "            'base_loss': 'field_error',\n",
    "            'spectral_loss': 'bsp',\n",
    "            'mu': 1.0,\n",
    "            'n_bins': N_BINS,\n",
    "            'epsilon': EPSILON,\n",
    "            'binning_mode': 'linear',\n",
    "            'signal_length': SIGNAL_LENGTH,\n",
    "            'cache_path': CACHE_PATH,\n",
    "            'lambda_k_mode': 'k_squared',  # Static k\u00b2 weighting for turbulence\n",
    "            'use_log': False,              # Standard energy (not log)\n",
    "            'use_output_norm': True,       # Per-batch output normalization\n",
    "            'use_minmax_norm': True,       # Per-sample min-max normalization\n",
    "            'loss_type': 'mspe'            # Mean Squared Percentage Error\n",
    "        },\n",
    "        'description': 'MSE + BSP (\u03bc=1.0, \u03bb_k=k\u00b2) with normalization - Fixed spectral loss'\n",
    "    },\n",
    "    \n",
    "    # Log-domain BSP with uniform weighting\n",
    "    'log_bsp': {\n",
    "        'loss_type': 'combined',\n",
    "        'loss_params': {\n",
    "            'base_loss': 'field_error',\n",
    "            'spectral_loss': 'bsp',\n",
    "            'mu': 1.0,\n",
    "            'n_bins': N_BINS,\n",
    "            'epsilon': EPSILON,\n",
    "            'binning_mode': 'linear',\n",
    "            'signal_length': SIGNAL_LENGTH,\n",
    "            'cache_path': CACHE_PATH,\n",
    "            'lambda_k_mode': 'uniform',    # Uniform \u03bb_k = 1 for all bins\n",
    "            'use_log': True,               # Log\u2081\u2080 transform of energies\n",
    "            'use_output_norm': True,       # Per-batch output normalization\n",
    "            'use_minmax_norm': True,       # Per-sample min-max normalization\n",
    "            'loss_type': 'l2_norm'         # L2 norm loss\n",
    "        },\n",
    "        'description': 'MSE + Log-BSP: log\u2081\u2080(E) with uniform weighting - Wide dynamic range'\n",
    "    },\n",
    "    \n",
    "    # Self-Adaptive BSP (trainable weights)\n",
    "    'sa_bsp': {\n",
    "        'loss_type': 'combined',\n",
    "        'loss_params': {\n",
    "            'base_loss': 'field_error',\n",
    "            'spectral_loss': 'sa_bsp',\n",
    "            'n_bins': N_BINS,\n",
    "            'adapt_mode': SA_ADAPT_MODE,   # Controlled by SA_ADAPT_MODE variable above\n",
    "            'init_weight': 1.0,\n",
    "            'epsilon': 1e-6,               # Increased from 1e-8 per paper ablation\n",
    "            'binning_mode': 'linear',\n",
    "            'signal_length': SIGNAL_LENGTH,\n",
    "            'cache_path': CACHE_PATH,\n",
    "            'lambda_k_mode': 'k_squared',  # k\u00b2 initialization for trainable weights\n",
    "            'use_log': False,              # Standard energy (not log)\n",
    "            'use_output_norm': True,       # Per-batch output normalization\n",
    "            'use_minmax_norm': True,       # Per-sample min-max normalization\n",
    "            'loss_type': 'mspe'            # Mean Squared Percentage Error\n",
    "        },\n",
    "        'description': f'MSE + SA-BSP ({SA_ADAPT_MODE}): Trainable \u03bb_k weights (init: k\u00b2) - Adaptive emphasis'\n",
    "    },\n",
    "    \n",
    "    # Self-Adaptive Log-BSP (trainable weights + log-domain)\n",
    "    'sa_log_bsp': {\n",
    "        'loss_type': 'combined',\n",
    "        'loss_params': {\n",
    "            'base_loss': 'field_error',\n",
    "            'spectral_loss': 'sa_bsp',\n",
    "            'n_bins': N_BINS,\n",
    "            'adapt_mode': SA_ADAPT_MODE,   # Controlled by SA_ADAPT_MODE variable above\n",
    "            'init_weight': 1.0,\n",
    "            'epsilon': 1e-6,               # Increased from 1e-8 per paper ablation\n",
    "            'binning_mode': 'linear',\n",
    "            'signal_length': SIGNAL_LENGTH,\n",
    "            'cache_path': CACHE_PATH,\n",
    "            'lambda_k_mode': 'uniform',    # Uniform initialization for log-domain\n",
    "            'use_log': True,               # Log\u2081\u2080 transform of energies\n",
    "            'use_output_norm': True,       # Per-batch output normalization\n",
    "            'use_minmax_norm': True,       # Per-sample min-max normalization\n",
    "            'loss_type': 'l2_norm'         # L2 norm loss\n",
    "        },\n",
    "        'description': f'MSE + SA-Log-BSP ({SA_ADAPT_MODE}): Trainable weights + log\u2081\u2080(E) - Adaptive + wide range'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Get selected loss configuration\n",
    "from configs.loss_config import LossConfig\n",
    "selected_loss_config = LossConfig.from_dict(LOSS_CONFIGS[LOSS_VARIANT])\n",
    "\n",
    "print(\"\u2713 Loss variant configuration loaded:\")\n",
    "print(f\"  Variant: {LOSS_VARIANT.upper()}\")\n",
    "if 'sa_' in LOSS_VARIANT:\n",
    "    print(f\"  SA Mode: {SA_ADAPT_MODE.upper()}\")\n",
    "print(f\"  Description: {selected_loss_config.description}\")\n",
    "print(f\"\\n  Parameters:\")\n",
    "for key, value in selected_loss_config.loss_params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3.7: Spectrum Comparison Utilities\n",
    "\n",
    "Helper functions for comparing energy spectra across different loss variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# SPECTRUM COMPARISON UTILITIES\n# ============================================================================\n\ndef compare_loss_variants(\n    trained_models_dict, \n    val_loader, \n    loss_variants=['bsp', 'log_bsp', 'sa_bsp', 'sa_log_bsp'],\n    model_arch='deeponet',\n    device='cuda' if torch.cuda.is_available() else 'cpu'\n):\n    \"\"\"\n    Compare energy spectra across different loss variants.\n    \n    Args:\n        trained_models_dict: Dictionary of trained models {variant: model}\n        val_loader: Validation data loader\n        loss_variants: List of loss variant names to compare\n        model_arch: Model architecture name\n        device: Device for inference\n        \n    Returns:\n        Dictionary of spectra {variant: {frequencies, energy_median, energy_p16, energy_p84}}\n    \"\"\"\n    from src.core.visualization.spectral_analysis import compute_unbinned_spectrum, compute_cached_true_spectrum\n    from configs.visualization_config import SPECTRUM_CACHE_FILENAME, CACHE_DIR\n    \n    print(\"Computing energy spectra for loss variant comparison...\")\n    \n    # Load true spectrum from cache\n    cache_path = f'{CACHE_DIR}/{SPECTRUM_CACHE_FILENAME}'\n    print(f\"Loading true spectrum from cache: {cache_path}\")\n    cached = np.load(cache_path)\n    k_true = cached['unbinned_frequencies']\n    E_true_median = cached['unbinned_energy_median']\n    E_true_p16 = cached['unbinned_energy_p16']\n    E_true_p84 = cached['unbinned_energy_p84']\n    print(f\"\u2713 True spectrum loaded ({len(k_true)} frequencies)\")\n    \n    spectra = {}\n    \n    # Store true spectrum\n    spectra['True'] = {\n        'frequencies': k_true,\n        'energy_median': E_true_median,\n        'energy_p16': E_true_p16,\n        'energy_p84': E_true_p84\n    }\n    \n    # Compute spectra for each loss variant\n    for variant in loss_variants:\n        key = f\"{model_arch}_{variant}\"\n        \n        if key not in trained_models_dict:\n            print(f\"  \u26a0\ufe0f  Skipping {variant}: model key '{key}' not found\")\n            continue\n        \n        model = trained_models_dict[key]\n        model.eval()\n        model.to(device)\n        \n        try:\n            # Collect predictions from all validation batches\n            all_preds = []\n            print(f\"  Processing {variant.upper()}...\", end='')\n            \n            with torch.no_grad():\n                for val_input, _ in val_loader:\n                    val_input = val_input.to(device)\n                    # Use appropriate forward method based on model type\n                    if hasattr(model, 'forward_sequence'):\n                        pred = model.forward_sequence(val_input)\n                    else:\n                        pred = model(val_input)\n                    all_preds.append(pred.cpu())\n            \n            # Stack predictions\n            all_preds_tensor = torch.cat(all_preds, dim=0)\n            \n            # Compute unbinned spectrum with percentile-based uncertainty\n            k_pred, E_pred_median, E_pred_p16, E_pred_p84 = compute_unbinned_spectrum(all_preds_tensor)\n            \n            spectra[variant] = {\n                'frequencies': k_pred,\n                'energy_median': E_pred_median,\n                'energy_p16': E_pred_p16,\n                'energy_p84': E_pred_p84\n            }\n            \n            print(f\" \u2713 ({all_preds_tensor.shape[0]} samples)\")\n        except Exception as e:\n            print(f\" \u274c Error: {e}\")\n            continue\n    \n    print(f\"\\n\u2713 Spectra computed for {len(spectra)} entries\\n\")\n    return spectra\n\n\ndef plot_spectrum_comparison(\n    spectra,\n    variants_to_plot=['bsp', 'log_bsp', 'sa_bsp', 'sa_log_bsp'],\n    title_suffix=\"\",\n    figsize=(14, 9)\n):\n    \"\"\"\n    Plot energy spectrum comparison for selected loss variants.\n    \n    Args:\n        spectra: Dictionary of spectra from compare_loss_variants()\n        variants_to_plot: List of variant names to include in plot\n        title_suffix: Additional text for plot title\n        figsize: Figure size tuple\n    \"\"\"\n    import matplotlib.pyplot as plt\n    \n    fig, ax = plt.subplots(figsize=figsize)\n    \n    # Color scheme\n    colors = {\n        'True': '#000000',\n        'bsp': '#ff7f0e',\n        'log_bsp': '#2ca02c',\n        'sa_bsp': '#d62728',\n        'sa_log_bsp': '#9467bd'\n    }\n    \n    # Plot ground truth\n    if 'True' in spectra:\n        data = spectra['True']\n        k = data['frequencies']\n        E_median = data['energy_median']\n        E_p16 = data['energy_p16']\n        E_p84 = data['energy_p84']\n        \n        ax.loglog(k, E_median, color=colors['True'], linewidth=3, \n                 label='True (Real Data)', zorder=10, alpha=0.9)\n        ax.fill_between(k, E_p16, E_p84,\n                        color=colors['True'], alpha=0.15, zorder=9,\n                        label='True (16th-84th percentile)')\n    \n    # Plot model predictions\n    label_map = {\n        'bsp': 'BSP (k\u00b2 weighting)',\n        'log_bsp': 'Log-BSP (uniform \u03bb)',\n        'sa_bsp': 'SA-BSP (adaptive \u03bb)',\n        'sa_log_bsp': 'SA-Log-BSP (adaptive + log)'\n    }\n    \n    for variant in variants_to_plot:\n        if variant not in spectra:\n            print(f\"  \u26a0\ufe0f  Skipping {variant}: not in spectra dictionary\")\n            continue\n        \n        data = spectra[variant]\n        k = data['frequencies']\n        E_median = data['energy_median']\n        E_p16 = data['energy_p16']\n        E_p84 = data['energy_p84']\n        \n        color = colors.get(variant, '#888888')\n        label = label_map.get(variant, variant.upper())\n        \n        # Plot median line\n        ax.loglog(k, E_median, color=color, linewidth=2.5, \n                 alpha=0.85, label=label, zorder=5)\n        \n        # Plot uncertainty band\n        ax.fill_between(k, E_p16, E_p84,\n                        color=color, alpha=0.12, zorder=4)\n    \n    # Configure plot\n    ax.set_xlabel('Frequency (normalized)', fontsize=14, fontweight='bold')\n    ax.set_ylabel('E(k) - Spectral Power', fontsize=14, fontweight='bold')\n    ax.set_title(f'Energy Spectrum Comparison: Loss Variants{title_suffix}', \n                fontsize=16, fontweight='bold')\n    ax.legend(fontsize=11, loc='best', framealpha=0.95)\n    ax.grid(True, alpha=0.3, which='both', linestyle='--')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\u2713 Spectrum comparison plot complete\")\n\n\ndef plot_side_by_side_spectra(\n    spectra_dict_1,\n    spectra_dict_2,\n    label_1=\"Model 1\",\n    label_2=\"Model 2\",\n    variants=['bsp', 'log_bsp', 'sa_bsp', 'sa_log_bsp'],\n    figsize=(20, 9)\n):\n    \"\"\"\n    Plot two spectrum comparisons side-by-side.\n    \n    Args:\n        spectra_dict_1: First spectra dictionary\n        spectra_dict_2: Second spectra dictionary\n        label_1: Label for first model\n        label_2: Label for second model\n        variants: List of variants to plot\n        figsize: Figure size tuple\n    \"\"\"\n    import matplotlib.pyplot as plt\n    \n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n    \n    colors = {\n        'True': '#000000',\n        'bsp': '#ff7f0e',\n        'log_bsp': '#2ca02c',\n        'sa_bsp': '#d62728',\n        'sa_log_bsp': '#9467bd'\n    }\n    \n    label_map = {\n        'bsp': 'BSP (k\u00b2)',\n        'log_bsp': 'Log-BSP',\n        'sa_bsp': 'SA-BSP',\n        'sa_log_bsp': 'SA-Log-BSP'\n    }\n    \n    for ax, spectra, title in [(ax1, spectra_dict_1, label_1), (ax2, spectra_dict_2, label_2)]:\n        # Plot ground truth\n        if 'True' in spectra:\n            data = spectra['True']\n            ax.loglog(data['frequencies'], data['energy_median'], \n                     color=colors['True'], linewidth=3, label='True', zorder=10)\n            ax.fill_between(data['frequencies'], data['energy_p16'], data['energy_p84'],\n                           color=colors['True'], alpha=0.15, zorder=9)\n        \n        # Plot variants\n        for variant in variants:\n            if variant in spectra:\n                data = spectra[variant]\n                color = colors.get(variant, '#888888')\n                label = label_map.get(variant, variant)\n                \n                ax.loglog(data['frequencies'], data['energy_median'],\n                         color=color, linewidth=2.5, alpha=0.85, label=label, zorder=5)\n                ax.fill_between(data['frequencies'], data['energy_p16'], data['energy_p84'],\n                               color=color, alpha=0.12, zorder=4)\n        \n        ax.set_xlabel('Frequency', fontsize=12, fontweight='bold')\n        ax.set_ylabel('E(k)', fontsize=12, fontweight='bold')\n        ax.set_title(title, fontsize=14, fontweight='bold')\n        ax.legend(fontsize=10, loc='best')\n        ax.grid(True, alpha=0.3, which='both', linestyle='--')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\u2713 Side-by-side spectrum comparison complete\")\n\n\nprint(\"\u2713 Spectrum comparison utilities loaded\")\nprint(\"\\nAvailable functions:\")\nprint(\"  \u2022 compare_loss_variants(trained_models_dict, val_loader, loss_variants, ...)\")\nprint(\"  \u2022 plot_spectrum_comparison(spectra, variants_to_plot, ...)\")\nprint(\"  \u2022 plot_side_by_side_spectra(spectra_1, spectra_2, label_1, label_2, ...)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3.9: Usage Example - Quick Loss Variant Comparison\n",
    "\n",
    "**Example workflow for comparing loss variants:**\n",
    "\n",
    "1. **Train with different variants** by changing `LOSS_VARIANT` in Cell 3.5:\n",
    "   ```python\n",
    "   # Train BSP\n",
    "   LOSS_VARIANT = 'bsp'\n",
    "   # Run training cells...\n",
    "   \n",
    "   # Train Log-BSP\n",
    "   LOSS_VARIANT = 'log_bsp'\n",
    "   # Run training cells...\n",
    "   \n",
    "   # Train SA-BSP\n",
    "   LOSS_VARIANT = 'sa_bsp'\n",
    "   SA_ADAPT_MODE = 'per-bin'\n",
    "   # Run training cells...\n",
    "   ```\n",
    "\n",
    "2. **Compare spectra** using the utility functions:\n",
    "   ```python\n",
    "   # After training multiple variants, compare their spectra\n",
    "   spectra = compare_loss_variants(\n",
    "       trained_models, \n",
    "       val_loader, \n",
    "       loss_variants=['bsp', 'log_bsp', 'sa_bsp'],\n",
    "       model_arch='deeponet'\n",
    "   )\n",
    "   \n",
    "   # Plot comparison\n",
    "   plot_spectrum_comparison(\n",
    "       spectra, \n",
    "       variants_to_plot=['bsp', 'log_bsp', 'sa_bsp'],\n",
    "       title_suffix=' - DeepONet Model'\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. **Side-by-side comparison** for different model architectures:\n",
    "   ```python\n",
    "   # Compare FNO vs DeepONet with same loss variant\n",
    "   plot_side_by_side_spectra(\n",
    "       spectra_fno, \n",
    "       spectra_deeponet,\n",
    "       label_1=\"FNO + BSP\",\n",
    "       label_2=\"DeepONet + BSP\",\n",
    "       variants=['bsp', 'log_bsp']\n",
    "   )\n",
    "   ```\n",
    "\n",
    "**Quick switches:**\n",
    "- Change `LOSS_VARIANT` to switch between 'bsp', 'log_bsp', 'sa_bsp', 'sa_log_bsp'\n",
    "- Change `SA_ADAPT_MODE` for SA-BSP variants: 'per-bin', 'global', 'combined'\n",
    "- Change `MODEL_ARCH` to try different architectures: 'deeponet', 'fno', 'unet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NOTE: Configuration has been moved to Cell 2 (above)\n",
    "# ============================================================================\n",
    "# This cell is kept for backward compatibility but is no longer needed.\n",
    "# All configuration (USE_CAUSAL_PADDING, DEEPONET_ACTIVATION, etc.) \n",
    "# is now defined in Cell 2 before data loading.\n",
    "\n",
    "print(\"\u26a0\ufe0f  This cell is deprecated - configuration is now in Cell 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Initialize Results Storage\n",
    "\n",
    "We'll train with all 6 loss types sequentially and store results for comparison:\n",
    "- **BASELINE**: Relative L2 loss only (MSE baseline)\n",
    "- **BSP**: MSE + fixed BSP loss with k\u00b2 weighting\n",
    "- **Log-BSP**: MSE + BSP with log\u2081\u2080 spectral energies (uniform weighting)\n",
    "- **SA-BSP-PERBIN**: MSE + 32 adaptive per-bin weights (negated gradients for frequency emphasis)\n",
    "- **SA-BSP-GLOBAL**: 2 adaptive weights (w_mse + w_bsp) with negated gradients for MSE/BSP balance\n",
    "- **SA-BSP-COMBINED**: 34 weights (w_mse + w_bsp + 32 per-bin) with full competitive dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import loss configurations\n",
    "from configs.loss_config import (\n",
    "    BASELINE_CONFIG, \n",
    "    BSP_CONFIG,\n",
    "    LOG_BSP_CONFIG,\n",
    "    SA_BSP_PERBIN_CONFIG,\n",
    "    SA_BSP_GLOBAL_CONFIG,\n",
    "    SA_BSP_COMBINED_CONFIG\n",
    ")\n",
    "from src.core.evaluation.loss_factory import create_loss\n",
    "\n",
    "# Loss configuration map\n",
    "loss_config_map = {\n",
    "    'baseline': BASELINE_CONFIG,\n",
    "    'bsp': BSP_CONFIG,\n",
    "    'log-bsp': LOG_BSP_CONFIG,\n",
    "    'sa-bsp-perbin': SA_BSP_PERBIN_CONFIG,\n",
    "    'sa-bsp-global': SA_BSP_GLOBAL_CONFIG,\n",
    "    'sa-bsp-combined': SA_BSP_COMBINED_CONFIG\n",
    "}\n",
    "\n",
    "# Storage dictionaries for results from all loss types\n",
    "all_training_results = {}  # Key: f\"{MODEL_ARCH}_{loss_type}\"\n",
    "all_trainers = {}\n",
    "trained_models = {}\n",
    "\n",
    "print(\"\u2713 Storage initialized for multi-loss training\")\n",
    "print(\"\\nWill train with 6 loss types:\")\n",
    "print(\"  1. BASELINE:\", BASELINE_CONFIG.description)\n",
    "print(\"  2. BSP:\", BSP_CONFIG.description)\n",
    "print(\"  3. LOG-BSP:\", LOG_BSP_CONFIG.description)\n",
    "print(\"  4. SA-BSP-PERBIN:\", SA_BSP_PERBIN_CONFIG.description)\n",
    "print(\"  5. SA-BSP-GLOBAL:\", SA_BSP_GLOBAL_CONFIG.description)\n",
    "print(\"  6. SA-BSP-COMBINED:\", SA_BSP_COMBINED_CONFIG.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Sequential Training with All Loss Types\n",
    "\n",
    "Train the same model architecture with all 6 loss functions sequentially:\n",
    "1. **BASELINE** - Pure MSE baseline\n",
    "2. **BSP** - Fixed spectral loss with k\u00b2 weighting\n",
    "3. **Log-BSP** - Spectral loss with log\u2081\u2080 energies and uniform weighting\n",
    "4. **SA-BSP-PERBIN** - 32 adaptive weights (emphasize hard frequency bins)\n",
    "5. **SA-BSP-GLOBAL** - 2 adaptive weights (learn MSE/BSP balance)\n",
    "6. **SA-BSP-COMBINED** - 34 adaptive weights (full competitive dynamics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train with all 6 loss types sequentially\nloss_types_to_train = ['baseline', 'bsp', 'log-bsp', 'sa-bsp-perbin', 'sa-bsp-global', 'sa-bsp-combined']\n\nfor LOSS_TYPE in loss_types_to_train:\n    print(f\"\\n{'='*70}\")\n    print(f\"Training {MODEL_ARCH.upper()} with {LOSS_TYPE.upper()} Loss\")\n    print(f\"{'='*70}\\n\")\n    \n    # Select loss configuration\n    selected_loss_config = loss_config_map[LOSS_TYPE]\n    print(f\"Loss config: {selected_loss_config.description}\")\n    \n    # Create loss function\n    criterion = create_loss(selected_loss_config)\n    print(f\"\u2713 Loss function created: {type(criterion).__name__}\")\n    \n    # Create FRESH model for this loss type (important!)\n    model_for_loss = create_model(MODEL_ARCH)\n    num_params = sum(p.numel() for p in model_for_loss.parameters() if p.requires_grad)\n    print(f\"\u2713 Fresh model created ({num_params:,} parameters)\")\n    \n    # Create training config\n    # Select optimizer based on architecture\n    # FNO has complex-valued Fourier layers incompatible with SOAP\n    optimizer_type = 'adam' if MODEL_ARCH == 'fno' else 'soap'\n    \n    config = TrainingConfig(\n        num_epochs=20,\n        learning_rate=1e-3,\n        optimizer_type=optimizer_type,  # Adam for FNO, SOAP for others\n        batch_size=BATCH_SIZE,\n        weight_decay=1e-4,\n        scheduler_type='cosine',\n        cosine_eta_min=1e-6,\n        eval_metrics=['field_error', 'spectrum_error'],\n        eval_frequency=1,\n        checkpoint_dir=f'checkpoints/{MODEL_ARCH}_{LOSS_TYPE}',\n        save_best=False,\n        save_latest=False,\n        device='cuda' if torch.cuda.is_available() else 'cpu',\n        num_workers=2,\n        verbose=True\n    )\n    \n    # Create trainer\n    trainer = SimpleTrainer(\n        model=model_for_loss,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        config=config,\n        loss_config=selected_loss_config,\n        experiment_name=f'{MODEL_ARCH}_{LOSS_TYPE}'\n    )\n    \n    print(f\"\u2713 Trainer initialized\")\n    print(f\"  Device: {trainer.device}\")\n    print(f\"  Optimizer: {type(trainer.optimizer).__name__}\")\n    \n    # Check for weight optimizer (SA-BSP variants only)\n    if 'sa-bsp' in LOSS_TYPE:\n        if trainer.weight_optimizer is not None:\n            adapt_mode = trainer.adapt_mode\n            print(f\"  Weight optimizer: \u2713 Created for SA-BSP ({adapt_mode} mode)\")\n        else:\n            print(f\"  \u26a0 WARNING: SA-BSP but no weight_optimizer!\")\n    \n    print(f\"\\n\ud83d\ude80 Starting training...\\n\")\n    \n    # Train\n    results = trainer.train()\n    \n    # Store results\n    key = f\"{MODEL_ARCH}_{LOSS_TYPE}\"\n    all_training_results[key] = results\n    all_trainers[key] = trainer\n    trained_models[key] = model_for_loss\n    \n    print(f\"\\n\u2705 {LOSS_TYPE.upper()} training complete!\")\n    print(f\"   Best val loss: {results['best_val_loss']:.6f}\")\n    print(f\"   Final val loss: {results['val_history'][-1]['loss']:.6f}\")\n\nprint(f\"\\n{'='*70}\")\nprint(f\"ALL TRAINING COMPLETE!\")\nprint(f\"{'='*70}\")\nprint(f\"Trained {len(all_training_results)} models with different loss functions\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Multi-Loss Training Comparison\n",
    "\n",
    "Compare training metrics across all 6 loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-loss comparison plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Color scheme for loss types\n",
    "colors = {\n",
    "    'baseline': '#1f77b4',       # Blue\n",
    "    'bsp': '#ff7f0e',             # Orange\n",
    "    'log-bsp': '#2ca02c',         # Green\n",
    "    'sa-bsp-perbin': '#d62728',   # Red\n",
    "    'sa-bsp-global': '#9467bd',   # Purple\n",
    "    'sa-bsp-combined': '#17becf'  # Cyan\n",
    "}\n",
    "linestyles = {\n",
    "    'baseline': '-', \n",
    "    'bsp': '--', \n",
    "    'log-bsp': '-.', \n",
    "    'sa-bsp-perbin': ':', \n",
    "    'sa-bsp-global': '-',\n",
    "    'sa-bsp-combined': '--'\n",
    "}\n",
    "markers = {\n",
    "    'baseline': 'o', \n",
    "    'bsp': 's', \n",
    "    'log-bsp': '^', \n",
    "    'sa-bsp-perbin': 'D', \n",
    "    'sa-bsp-global': 'v',\n",
    "    'sa-bsp-combined': 'p'\n",
    "}\n",
    "\n",
    "for loss_type in ['baseline', 'bsp', 'log-bsp', 'sa-bsp-perbin', 'sa-bsp-global', 'sa-bsp-combined']:\n",
    "    key = f\"{MODEL_ARCH}_{loss_type}\"\n",
    "    results = all_training_results[key]\n",
    "    \n",
    "    # Extract metrics\n",
    "    val_losses = [h['loss'] for h in results['val_history']]\n",
    "    val_field_errors = [h['field_error'] for h in results['val_history']]\n",
    "    val_spectrum_errors = [h['spectrum_error'] for h in results['val_history']]\n",
    "    epochs = range(1, len(val_losses) + 1)\n",
    "    \n",
    "    # Create label with short name\n",
    "    label_map = {\n",
    "        'baseline': 'BASELINE',\n",
    "        'bsp': 'BSP',\n",
    "        'log-bsp': 'Log-BSP',\n",
    "        'sa-bsp-perbin': 'SA-BSP (Per-bin)',\n",
    "        'sa-bsp-global': 'SA-BSP (Global)',\n",
    "        'sa-bsp-combined': 'SA-BSP (Combined)'\n",
    "    }\n",
    "    label = label_map[loss_type]\n",
    "    \n",
    "    # Plot on all 3 axes\n",
    "    axes[0].plot(epochs, val_losses, label=label, \n",
    "                color=colors[loss_type], linestyle=linestyles[loss_type],\n",
    "                linewidth=2, alpha=0.9, marker=markers[loss_type], markersize=4, markevery=5)\n",
    "    \n",
    "    axes[1].plot(epochs, val_field_errors, label=label,\n",
    "                color=colors[loss_type], linestyle=linestyles[loss_type],\n",
    "                linewidth=2, alpha=0.9, marker=markers[loss_type], markersize=4, markevery=5)\n",
    "    \n",
    "    axes[2].plot(epochs, val_spectrum_errors, label=label,\n",
    "                color=colors[loss_type], linestyle=linestyles[loss_type],\n",
    "                linewidth=2, alpha=0.9, marker=markers[loss_type], markersize=4, markevery=5)\n",
    "\n",
    "# Configure axes with LOG SCALE on y-axis\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Validation Loss', fontsize=12)\n",
    "axes[0].set_title('Validation Loss Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_yscale('log')  # LOG SCALE\n",
    "axes[0].legend(fontsize=9, loc='best')\n",
    "axes[0].grid(True, alpha=0.3, which='both')\n",
    "\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Field Error', fontsize=12)\n",
    "axes[1].set_title('Field Error (Real Space)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_yscale('log')  # LOG SCALE\n",
    "axes[1].legend(fontsize=9, loc='best')\n",
    "axes[1].grid(True, alpha=0.3, which='both')\n",
    "\n",
    "axes[2].set_xlabel('Epoch', fontsize=12)\n",
    "axes[2].set_ylabel('Spectrum Error', fontsize=12)\n",
    "axes[2].set_title('Spectrum Error (Frequency Space)', fontsize=14, fontweight='bold')\n",
    "axes[2].set_yscale('log')  # LOG SCALE\n",
    "axes[2].legend(fontsize=9, loc='best')\n",
    "axes[2].grid(True, alpha=0.3, which='both')\n",
    "\n",
    "plt.suptitle(f'{MODEL_ARCH.upper()}: Loss Function Comparison (6 Variants)', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics table\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Final Metrics Summary\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"{'Loss Type':<25} {'Val Loss':<12} {'Field Error':<15} {'Spectrum Error':<15}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for loss_type in ['baseline', 'bsp', 'log-bsp', 'sa-bsp-perbin', 'sa-bsp-global', 'sa-bsp-combined']:\n",
    "    key = f\"{MODEL_ARCH}_{loss_type}\"\n",
    "    results = all_training_results[key]\n",
    "    final_val = results['val_history'][-1]\n",
    "    \n",
    "    label = loss_type.upper()\n",
    "    print(f\"{label:<25} {final_val['loss']:<12.6f} \"\n",
    "          f\"{final_val['field_error']:<15.6f} {final_val['spectrum_error']:<15.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Spectral Bias Visualization (Energy Spectrum)\n",
    "\n",
    "Visualize E(k) vs wavenumber to identify spectral bias in trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.fft as fft\n",
    "from src.core.visualization.spectral_analysis import compute_unbinned_spectrum, compute_cached_true_spectrum\n",
    "from configs.visualization_config import SPECTRUM_CACHE_FILENAME, CACHE_DIR\n",
    "\n",
    "# Get validation batch for energy spectrum analysis\n",
    "print(\"Computing energy spectra for all trained models...\")\n",
    "\n",
    "# Check if models have been trained\n",
    "if 'trained_models' not in globals() or len(trained_models) == 0:\n",
    "    print(\"\\n\u26a0\ufe0f  WARNING: No trained models found!\")\n",
    "    print(\"   Please run Cell 12 (training) first before running this cell.\")\n",
    "    print(\"   This cell requires the 'trained_models' dictionary to be populated.\\n\")\n",
    "else:\n",
    "    print(f\"\u2713 Found {len(trained_models)} trained models\")\n",
    "    print(f\"  Keys: {list(trained_models.keys())}\\n\")\n",
    "\n",
    "val_batch_input, val_batch_target = next(iter(val_loader))\n",
    "\n",
    "# Move to device for inference\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "val_batch_input = val_batch_input.to(device)\n",
    "val_batch_target = val_batch_target.to(device)\n",
    "\n",
    "# Compute ground truth spectrum with percentile-based uncertainty bands from cache\n",
    "cache_path = f'{CACHE_DIR}/{SPECTRUM_CACHE_FILENAME}'\n",
    "print(f\"Loading true spectrum from cache: {cache_path}\")\n",
    "cached = np.load(cache_path)\n",
    "k_true = cached['unbinned_frequencies']  # Full FFT resolution (~2000 frequencies)\n",
    "E_true_median = cached['unbinned_energy_median']  # Median (50th percentile)\n",
    "E_true_p16 = cached['unbinned_energy_p16']        # Lower bound (16th percentile \u2248 -1\u03c3)\n",
    "E_true_p84 = cached['unbinned_energy_p84']        # Upper bound (84th percentile \u2248 +1\u03c3)\n",
    "print(f\"\u2713 True spectrum loaded ({len(k_true)} frequencies, unbinned)\")\n",
    "print(f\"  Using percentile-based uncertainty bands (16th-84th \u2248 \u00b11\u03c3)\")\n",
    "\n",
    "# Collect ALL validation predictions for uncertainty bands\n",
    "print(\"\\nComputing unbinned spectra with percentile-based uncertainty bands for all models...\")\n",
    "spectra = {}\n",
    "\n",
    "# Store true spectrum with percentile uncertainty bounds\n",
    "spectra['True'] = {\n",
    "    'frequencies': k_true,\n",
    "    'energy_median': E_true_median,\n",
    "    'energy_p16': E_true_p16,\n",
    "    'energy_p84': E_true_p84\n",
    "}\n",
    "\n",
    "for loss_type in ['baseline', 'bsp', 'log-bsp', 'sa-bsp-perbin', 'sa-bsp-global', 'sa-bsp-combined']:\n",
    "    key = f\"{MODEL_ARCH}_{loss_type}\"\n",
    "    \n",
    "    # Check if model exists\n",
    "    if key not in trained_models:\n",
    "        print(f\"  \u26a0\ufe0f  Skipping {loss_type.upper()}: model key '{key}' not found in trained_models\")\n",
    "        continue\n",
    "    \n",
    "    model_trained = trained_models[key]\n",
    "    model_trained.eval()\n",
    "    model_trained.to(device)\n",
    "    \n",
    "    try:\n",
    "        # Collect predictions from ALL validation batches for uncertainty bands\n",
    "        all_preds = []\n",
    "        print(f\"  Processing {loss_type.upper()}...\", end='')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for val_input, _ in val_loader:\n",
    "                val_input = val_input.to(device)\n",
    "                # Use appropriate forward method based on model type\n",
    "                if hasattr(model_trained, 'forward_sequence'):\n",
    "                    pred = model_trained.forward_sequence(val_input)\n",
    "                else:\n",
    "                    pred = model_trained(val_input)\n",
    "                all_preds.append(pred.cpu())\n",
    "        \n",
    "        # Stack all predictions: [total_val_samples, C, T]\n",
    "        all_preds_tensor = torch.cat(all_preds, dim=0)\n",
    "        \n",
    "        # Compute unbinned spectrum with percentile-based uncertainty bands\n",
    "        k_pred, E_pred_median, E_pred_p16, E_pred_p84 = compute_unbinned_spectrum(all_preds_tensor)\n",
    "        \n",
    "        # Create display label\n",
    "        label_map = {\n",
    "            'baseline': 'BASELINE',\n",
    "            'bsp': 'BSP',\n",
    "            'log-bsp': 'Log-BSP',\n",
    "            'sa-bsp-perbin': 'SA-BSP (Per-bin)',\n",
    "            'sa-bsp-global': 'SA-BSP (Global)',\n",
    "            'sa-bsp-combined': 'SA-BSP (Combined)'\n",
    "        }\n",
    "        spec_key = f\"{MODEL_ARCH.upper()} + {label_map[loss_type]}\"\n",
    "        \n",
    "        spectra[spec_key] = {\n",
    "            'frequencies': k_pred,\n",
    "            'energy_median': E_pred_median,\n",
    "            'energy_p16': E_pred_p16,\n",
    "            'energy_p84': E_pred_p84\n",
    "        }\n",
    "        \n",
    "        print(f\" \u2713 ({all_preds_tensor.shape[0]} samples)\")\n",
    "    except Exception as e:\n",
    "        print(f\" \u274c Error: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n\u2713 Spectra computed for {len(spectra)} entries with percentile-based uncertainty bands\\n\")\n",
    "\n",
    "# Plot energy spectrum with percentile-based uncertainty bands (safe for log scale!)\n",
    "fig, ax = plt.subplots(figsize=(14, 9))\n",
    "\n",
    "# Color scheme for loss types\n",
    "colors_plot = {\n",
    "    'True': '#000000',  # Black for ground truth\n",
    "    'baseline': '#1f77b4',\n",
    "    'bsp': '#ff7f0e',\n",
    "    'log-bsp': '#2ca02c',\n",
    "    'sa-bsp-perbin': '#d62728',\n",
    "    'sa-bsp-global': '#9467bd',\n",
    "    'sa-bsp-combined': '#17becf'\n",
    "}\n",
    "\n",
    "# Plot ground truth with uncertainty band (black)\n",
    "if 'True' in spectra:\n",
    "    data = spectra['True']\n",
    "    k = data['frequencies']\n",
    "    E_median = data['energy_median']\n",
    "    E_p16 = data['energy_p16']\n",
    "    E_p84 = data['energy_p84']\n",
    "    \n",
    "    # Plot median line\n",
    "    ax.loglog(k, E_median, color=colors_plot['True'], linewidth=3, \n",
    "             label='True (Real Data)', zorder=10, alpha=0.9)\n",
    "    \n",
    "    # Plot percentile-based uncertainty band (16th-84th percentiles \u2248 \u00b11\u03c3)\n",
    "    # These are GUARANTEED to be positive \u2192 safe for log scale!\n",
    "    ax.fill_between(k, E_p16, E_p84,\n",
    "                     color=colors_plot['True'], alpha=0.15, zorder=9,\n",
    "                     label='True (16th-84th percentile)')\n",
    "\n",
    "# Plot model predictions with percentile-based uncertainty bands\n",
    "for loss_type in ['baseline', 'bsp', 'log-bsp', 'sa-bsp-perbin', 'sa-bsp-global', 'sa-bsp-combined']:\n",
    "    label_map = {\n",
    "        'baseline': 'BASELINE',\n",
    "        'bsp': 'BSP',\n",
    "        'log-bsp': 'Log-BSP',\n",
    "        'sa-bsp-perbin': 'SA-BSP (Per-bin)',\n",
    "        'sa-bsp-global': 'SA-BSP (Global)',\n",
    "        'sa-bsp-combined': 'SA-BSP (Combined)'\n",
    "    }\n",
    "    label_key = f\"{MODEL_ARCH.upper()} + {label_map[loss_type]}\"\n",
    "    \n",
    "    # Check if spectrum exists before plotting\n",
    "    if label_key not in spectra:\n",
    "        print(f\"  \u26a0\ufe0f  Skipping plot for {loss_type.upper()}: '{label_key}' not in spectra dictionary\")\n",
    "        continue\n",
    "    \n",
    "    data = spectra[label_key]\n",
    "    k = data['frequencies']\n",
    "    E_median = data['energy_median']\n",
    "    E_p16 = data['energy_p16']\n",
    "    E_p84 = data['energy_p84']\n",
    "    \n",
    "    color = colors_plot[loss_type]\n",
    "    \n",
    "    # Plot median line\n",
    "    ax.loglog(k, E_median, color=color, linewidth=2.5, \n",
    "             alpha=0.85, label=label_key, zorder=5)\n",
    "    \n",
    "    # Plot percentile-based uncertainty band (guaranteed positive for log scale)\n",
    "    ax.fill_between(k, E_p16, E_p84,\n",
    "                     color=color, alpha=0.12, zorder=4)\n",
    "\n",
    "# Configure plot\n",
    "ax.set_xlabel('Frequency (normalized)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('E(k) - Spectral Power', fontsize=14, fontweight='bold')\n",
    "ax.set_title(f'Energy Spectrum Comparison with Percentile Uncertainty Bands\\n{MODEL_ARCH.upper()} Model (6 Loss Variants)', \n",
    "            fontsize=16, fontweight='bold')\n",
    "ax.legend(fontsize=10, loc='best', framealpha=0.95, ncol=1)\n",
    "ax.grid(True, alpha=0.3, which='both', linestyle='--')\n",
    "\n",
    "# Set nice axis limits\n",
    "ax.set_xlim(k_true.min() * 0.9, k_true.max() * 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\u2713 Energy spectrum plot complete\")\n",
    "print(f\"  \u2022 Unbinned spectrum: Full FFT resolution (~{len(k_true)} frequencies)\")\n",
    "print(f\"  \u2022 Uncertainty bands: 16th-84th percentiles (\u2248 \u00b11\u03c3) across all validation samples\")\n",
    "print(f\"  \u2022 Percentiles are ALWAYS positive \u2192 safe for log-scale display!\")\n",
    "print(f\"  \u2022 This visualization shows spectral bias: deviation from ground truth at high frequencies\")\n",
    "print(f\"  \u2022 Log-BSP and SA-BSP variants should show better high-frequency matching than baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Spectral Bias Quantification\n",
    "\n",
    "Compute spectral bias metrics to quantify how well each model captures high-frequency content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core.visualization.spectral_analysis import compute_spectral_bias_metric\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SPECTRAL BIAS METRICS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nQuantifies how well each model captures different frequency ranges.\")\n",
    "print(\"Spectral Bias Ratio = High Freq Error / Low Freq Error\")\n",
    "print(\"  - Ratio > 2.0: Significant spectral bias (struggles with high frequencies)\")\n",
    "print(\"  - Ratio > 1.5: Moderate spectral bias\")\n",
    "print(\"  - Ratio \u2264 1.5: Low spectral bias (captures frequencies well)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compute metrics for each trained model\n",
    "spectral_metrics = {}\n",
    "\n",
    "for loss_type in ['baseline', 'bsp', 'log-bsp', 'sa-bsp-perbin', 'sa-bsp-global', 'sa-bsp-combined']:\n",
    "    key = f\"{MODEL_ARCH}_{loss_type}\"\n",
    "    model_trained = trained_models[key]\n",
    "    model_trained.eval()\n",
    "    model_trained.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Use appropriate forward method based on model type\n",
    "        if hasattr(model_trained, 'forward_sequence'):\n",
    "            pred = model_trained.forward_sequence(val_input)\n",
    "        else:\n",
    "            pred = model_trained(val_input)\n",
    "    \n",
    "    metrics = compute_spectral_bias_metric(pred.cpu(), val_target.cpu(), n_bins=32)\n",
    "    spectral_metrics[loss_type] = metrics\n",
    "    \n",
    "    label_map = {\n",
    "        'baseline': 'BASELINE',\n",
    "        'bsp': 'BSP',\n",
    "        'log-bsp': 'Log-BSP',\n",
    "        'sa-bsp-perbin': 'SA-BSP (Per-bin)',\n",
    "        'sa-bsp-global': 'SA-BSP (Global)',\n",
    "        'sa-bsp-combined': 'SA-BSP (Combined)'\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{MODEL_ARCH.upper()} + {label_map[loss_type]}:\")\n",
    "    print(f\"  Low frequency error:   {metrics['low_freq_error']:.6f}\")\n",
    "    print(f\"  Mid frequency error:   {metrics['mid_freq_error']:.6f}\")\n",
    "    print(f\"  High frequency error:  {metrics['high_freq_error']:.6f}\")\n",
    "    print(f\"  Spectral bias ratio:   {metrics['spectral_bias_ratio']:.4f}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    if metrics['spectral_bias_ratio'] > 2.0:\n",
    "        print(f\"  \u2192 \u26a0\ufe0f  SIGNIFICANT spectral bias detected!\")\n",
    "        print(f\"     Model struggles with high-frequency content\")\n",
    "    elif metrics['spectral_bias_ratio'] > 1.5:\n",
    "        print(f\"  \u2192 \u26a1 MODERATE spectral bias\")\n",
    "        print(f\"     Some difficulty with high frequencies\")\n",
    "    else:\n",
    "        print(f\"  \u2192 \u2705 LOW spectral bias\")\n",
    "        print(f\"     Model captures frequency content well\")\n",
    "\n",
    "# Create comparison visualization\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Spectral Bias Comparison\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Bar plot 1: Frequency errors\n",
    "loss_types = ['baseline', 'bsp', 'log-bsp', 'sa-bsp-perbin', 'sa-bsp-global', 'sa-bsp-combined']\n",
    "x = np.arange(len(loss_types))\n",
    "width = 0.2\n",
    "\n",
    "low_errors = [spectral_metrics[lt]['low_freq_error'] for lt in loss_types]\n",
    "mid_errors = [spectral_metrics[lt]['mid_freq_error'] for lt in loss_types]\n",
    "high_errors = [spectral_metrics[lt]['high_freq_error'] for lt in loss_types]\n",
    "\n",
    "ax1.bar(x - width, low_errors, width, label='Low Freq', color='#2ca02c', alpha=0.8)\n",
    "ax1.bar(x, mid_errors, width, label='Mid Freq', color='#ff7f0e', alpha=0.8)\n",
    "ax1.bar(x + width, high_errors, width, label='High Freq', color='#d62728', alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('Loss Type', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Frequency Error', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Frequency Range Errors', fontsize=14, fontweight='bold')\n",
    "ax1.set_yscale('log')  # LOG SCALE\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(['BASE', 'BSP', 'Log-BSP', 'SA-Per', 'SA-Glob', 'SA-Comb'], rotation=15, ha='right')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3, axis='y', which='both')\n",
    "\n",
    "# Bar plot 2: Spectral bias ratio\n",
    "bias_ratios = [spectral_metrics[lt]['spectral_bias_ratio'] for lt in loss_types]\n",
    "colors_bars = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#17becf']\n",
    "\n",
    "bars = ax2.bar(x, bias_ratios, color=colors_bars, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Add threshold lines\n",
    "ax2.axhline(y=2.0, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Significant bias threshold')\n",
    "ax2.axhline(y=1.5, color='orange', linestyle='--', linewidth=2, alpha=0.7, label='Moderate bias threshold')\n",
    "\n",
    "ax2.set_xlabel('Loss Type', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Spectral Bias Ratio', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Spectral Bias Ratio (High/Low)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(['BASE', 'BSP', 'Log-BSP', 'SA-Per', 'SA-Glob', 'SA-Comb'], rotation=15, ha='right')\n",
    "ax2.legend(fontsize=10, loc='upper right')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, ratio) in enumerate(zip(bars, bias_ratios)):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "            f'{ratio:.2f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.suptitle(f'{MODEL_ARCH.upper()}: Spectral Bias Analysis (6 Loss Variants)', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"\u2705 Spectral bias analysis complete!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(\"\\nKey Findings:\")\n",
    "print(\"  \u2022 Baseline: Pure MSE - typically shows significant spectral bias\")\n",
    "print(\"  \u2022 BSP: Fixed spectral loss with k\u00b2 weighting - moderate improvement\")\n",
    "print(\"  \u2022 Log-BSP: Log-domain spectral loss - addresses wide dynamic range\")\n",
    "print(\"  \u2022 SA-BSP (Per-bin): Adaptive per-bin weights - emphasize hard frequencies\")\n",
    "print(\"  \u2022 SA-BSP (Global): Adaptive MSE/BSP balance - optimize overall trade-off\")\n",
    "print(\"  \u2022 SA-BSP (Combined): Full competitive dynamics - most expressive approach\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. \u2713 Loading real CDON data with proper normalization\n",
    "2. \u2713 Creating neural operator models (DeepONet, FNO, UNet)\n",
    "3. \u2713 **Sequential training with all 6 loss functions**:\n",
    "   - **BASELINE**: Relative L2 loss only (MSE baseline)\n",
    "   - **BSP**: MSE + fixed BSP loss with k\u00b2 weighting\n",
    "   - **Log-BSP**: MSE + BSP with log\u2081\u2080 spectral energies (uniform weighting)\n",
    "   - **SA-BSP (Per-bin)**: MSE + 32 adaptive per-bin weights (negated gradients for frequency emphasis)\n",
    "   - **SA-BSP (Global)**: MSE + 2 adaptive weights (w_mse + w_bsp, negated gradients for MSE/BSP balance)\n",
    "   - **SA-BSP (Combined)**: MSE + 34 weights (w_mse + w_bsp + 32 per-bin, all negated gradients for full competitive dynamics)\n",
    "4. \u2713 **Multi-loss comparison plots** showing training metrics\n",
    "5. \u2713 **Energy spectrum visualization** (E(k) vs wavenumber) to identify spectral bias\n",
    "6. \u2713 **Spectral bias quantification** with metrics and comparison plots\n",
    "\n",
    "**Key Results:**\n",
    "- All 6 loss types trained on the same model architecture\n",
    "- Direct comparison shows which loss function best mitigates spectral bias\n",
    "- Energy spectrum plot reveals how well each model captures high-frequency content\n",
    "- Quantitative metrics identify spectral bias ratio for each approach\n",
    "\n",
    "**SA-PINNs Implementation:**\n",
    "- **Per-bin mode**: Uses negated gradients (ascent) to emphasize difficult frequency bins\n",
    "- **Global mode**: Uses negated gradients (ascent) to learn optimal MSE/BSP balance via competitive dynamics\n",
    "- **Combined mode**: Full competitive dynamics with all weights (w_mse, w_bsp, and 32 per-bin) using negated gradients\n",
    "\n",
    "**Experiment with different configurations:**\n",
    "- **Cell 0**: Run to force reload modules after code changes\n",
    "- **Cell 3**: Change `MODEL_ARCH` to try different models ('deeponet', 'fno', 'unet')\n",
    "- **Cell 5**: Adjust hyperparameters (epochs, learning rate, etc.) in TrainingConfig\n",
    "- Run all cells sequentially to train and compare all 6 loss types automatically!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}