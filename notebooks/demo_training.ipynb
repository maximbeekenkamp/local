{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Operator Training Demo: CDON Dataset\n",
    "\n",
    "This notebook demonstrates end-to-end training of neural operator models (DeepONet, FNO, UNet) on the CDON dataset.\n",
    "\n",
    "**Features:**\n",
    "- Trains on **real CDON data**\n",
    "- **Configurable loss functions** (Baseline, BSP, SA-BSP)\n",
    "- Minimal custom code - reuses existing codebase\n",
    "- Includes visualizations of training progress\n",
    "- Compatible with Google Colab\n",
    "\n",
    "**Models available:**\n",
    "- `deeponet`: Branch-trunk architecture (~235K params)\n",
    "- `fno`: Fourier Neural Operator (~261K params)\n",
    "- `unet`: Encoder-decoder with skip connections (~249K params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup & Imports (Colab-Ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab setup\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure we're in /content\n",
    "try:\n",
    "    os.chdir('/content')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Clone repository if running in Colab\n",
    "repo_path = Path('/content/local')\n",
    "if not repo_path.exists():\n",
    "    print(\"ðŸ“¥ Cloning repository...\")\n",
    "    !git clone https://github.com/maximbeekenkamp/local.git\n",
    "    print(\"âœ… Repository cloned\")\n",
    "else:\n",
    "    print(\"âœ… Repository exists\")\n",
    "\n",
    "# Change to repo directory\n",
    "try:\n",
    "    os.chdir('/content/local')\n",
    "    print(f\"âœ… Changed to: {os.getcwd()}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Install dependencies\n",
    "print(\"\\nðŸ“¦ Installing dependencies...\")\n",
    "!pip install -r requirements.txt -q\n",
    "print(\"âœ… Dependencies installed\")\n",
    "\n",
    "# Standard imports\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Project imports\n",
    "from src.core.data_processing.cdon_dataset import CDONDataset\n",
    "from src.core.data_processing.cdon_transforms import CDONNormalization\n",
    "from src.core.models.model_factory import create_model\n",
    "from src.core.training.simple_trainer import SimpleTrainer\n",
    "from configs.training_config import TrainingConfig\n",
    "\n",
    "print(\"\\nâœ“ Imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Load Real CDON Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get project root\n",
    "project_root = Path.cwd()\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Data directory\n",
    "DATA_DIR = project_root / 'CDONData'\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "\n",
    "# Create normalization object (required by CDONDataset)\n",
    "stats_path = project_root / 'configs' / 'cdon_stats.json'\n",
    "print(f\"Loading stats from: {stats_path}\")\n",
    "normalizer = CDONNormalization(stats_path=str(stats_path))\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CDONDataset(\n",
    "    data_dir=str(DATA_DIR),\n",
    "    split='train',\n",
    "    normalize=normalizer  # Pass normalizer object, not boolean\n",
    ")\n",
    "\n",
    "val_dataset = CDONDataset(\n",
    "    data_dir=str(DATA_DIR),\n",
    "    split='test',\n",
    "    normalize=normalizer\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Data loaded successfully\")\n",
    "print(f\"  Train samples: {len(train_dataset)}\")\n",
    "print(f\"  Val samples: {len(val_dataset)}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# Inspect a sample\n",
    "sample_input, sample_target = train_dataset[0]\n",
    "print(f\"\\nSample shapes:\")\n",
    "print(f\"  Input: {sample_input.shape}\")\n",
    "print(f\"  Target: {sample_target.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Choose Model Architecture\n",
    "\n",
    "**Change `MODEL_ARCH` to try different models:**\n",
    "- `'deeponet'`: Branch-trunk architecture\n",
    "- `'fno'`: Fourier Neural Operator\n",
    "- `'unet'`: U-Net encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model architecture\n",
    "MODEL_ARCH = 'deeponet'  # Options: 'deeponet', 'fno', 'unet'\n",
    "\n",
    "model = create_model(MODEL_ARCH)\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"âœ“ Created {MODEL_ARCH.upper()} model\")\n",
    "print(f\"  Parameters: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Summary\n\nThis notebook demonstrated:\n1. âœ“ Loading real CDON data with proper normalization\n2. âœ“ Creating neural operator models (DeepONet, FNO, UNet)\n3. âœ“ **Sequential training with all 3 loss functions** (Baseline, BSP, SA-BSP)\n4. âœ“ **Multi-loss comparison plots** showing training metrics\n5. âœ“ **Energy spectrum visualization** (E(k) vs wavenumber) to identify spectral bias\n6. âœ“ **Spectral bias quantification** with metrics and comparison plots\n\n**Key Results:**\n- All 3 loss types trained on the same model architecture\n- Direct comparison shows which loss function best mitigates spectral bias\n- Energy spectrum plot reveals how well each model captures high-frequency content\n- Quantitative metrics identify spectral bias ratio for each approach\n\n**Experiment with different configurations:**\n- **Cell 3**: Change `MODEL_ARCH` to try different models ('deeponet', 'fno', 'unet')\n- **Cell 5**: Adjust hyperparameters (epochs, learning rate, etc.) in TrainingConfig\n- Run all cells sequentially to train and compare all loss types automatically!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from src.core.visualization.spectral_analysis import compute_spectral_bias_metric\n\nprint(\"=\"*70)\nprint(\"SPECTRAL BIAS METRICS\")\nprint(\"=\"*70)\nprint(\"\\nQuantifies how well each model captures different frequency ranges.\")\nprint(\"Spectral Bias Ratio = High Freq Error / Low Freq Error\")\nprint(\"  - Ratio > 2.0: Significant spectral bias (struggles with high frequencies)\")\nprint(\"  - Ratio > 1.5: Moderate spectral bias\")\nprint(\"  - Ratio â‰¤ 1.5: Low spectral bias (captures frequencies well)\")\nprint(\"=\"*70)\n\n# Compute metrics for each trained model\nspectral_metrics = {}\n\nfor loss_type in ['baseline', 'bsp', 'sa-bsp']:\n    key = f\"{MODEL_ARCH}_{loss_type}\"\n    model_trained = trained_models[key]\n    model_trained.eval()\n    model_trained.to(device)\n    \n    with torch.no_grad():\n        pred = model_trained(val_input)\n    \n    metrics = compute_spectral_bias_metric(pred.cpu(), val_target.cpu(), n_bins=32)\n    spectral_metrics[loss_type] = metrics\n    \n    print(f\"\\n{MODEL_ARCH.upper()} + {loss_type.upper()} Loss:\")\n    print(f\"  Low frequency error:   {metrics['low_freq_error']:.6f}\")\n    print(f\"  Mid frequency error:   {metrics['mid_freq_error']:.6f}\")\n    print(f\"  High frequency error:  {metrics['high_freq_error']:.6f}\")\n    print(f\"  Spectral bias ratio:   {metrics['spectral_bias_ratio']:.4f}\")\n    \n    # Interpretation\n    if metrics['spectral_bias_ratio'] > 2.0:\n        print(f\"  â†’ âš ï¸  SIGNIFICANT spectral bias detected!\")\n        print(f\"     Model struggles with high-frequency content\")\n    elif metrics['spectral_bias_ratio'] > 1.5:\n        print(f\"  â†’ âš¡ MODERATE spectral bias\")\n        print(f\"     Some difficulty with high frequencies\")\n    else:\n        print(f\"  â†’ âœ… LOW spectral bias\")\n        print(f\"     Model captures frequency content well\")\n\n# Create comparison visualization\nprint(f\"\\n{'='*70}\")\nprint(\"Spectral Bias Comparison\")\nprint(f\"{'='*70}\")\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\n# Bar plot 1: Frequency errors\nx = np.arange(3)\nwidth = 0.25\n\nloss_types = ['baseline', 'bsp', 'sa-bsp']\nlow_errors = [spectral_metrics[lt]['low_freq_error'] for lt in loss_types]\nmid_errors = [spectral_metrics[lt]['mid_freq_error'] for lt in loss_types]\nhigh_errors = [spectral_metrics[lt]['high_freq_error'] for lt in loss_types]\n\nax1.bar(x - width, low_errors, width, label='Low Freq', color='#2ca02c', alpha=0.8)\nax1.bar(x, mid_errors, width, label='Mid Freq', color='#ff7f0e', alpha=0.8)\nax1.bar(x + width, high_errors, width, label='High Freq', color='#d62728', alpha=0.8)\n\nax1.set_xlabel('Loss Type', fontsize=12, fontweight='bold')\nax1.set_ylabel('Frequency Error', fontsize=12, fontweight='bold')\nax1.set_title('Frequency Range Errors', fontsize=14, fontweight='bold')\nax1.set_xticks(x)\nax1.set_xticklabels([lt.upper() for lt in loss_types])\nax1.legend(fontsize=11)\nax1.grid(True, alpha=0.3, axis='y')\n\n# Bar plot 2: Spectral bias ratio\nbias_ratios = [spectral_metrics[lt]['spectral_bias_ratio'] for lt in loss_types]\ncolors_bars = [colors_plot[lt] for lt in loss_types]\n\nbars = ax2.bar(x, bias_ratios, color=colors_bars, alpha=0.8, edgecolor='black', linewidth=1.5)\n\n# Add threshold lines\nax2.axhline(y=2.0, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Significant bias threshold')\nax2.axhline(y=1.5, color='orange', linestyle='--', linewidth=2, alpha=0.7, label='Moderate bias threshold')\n\nax2.set_xlabel('Loss Type', fontsize=12, fontweight='bold')\nax2.set_ylabel('Spectral Bias Ratio', fontsize=12, fontweight='bold')\nax2.set_title('Spectral Bias Ratio (High/Low)', fontsize=14, fontweight='bold')\nax2.set_xticks(x)\nax2.set_xticklabels([lt.upper() for lt in loss_types])\nax2.legend(fontsize=10, loc='upper right')\nax2.grid(True, alpha=0.3, axis='y')\n\n# Add value labels on bars\nfor i, (bar, ratio) in enumerate(zip(bars, bias_ratios)):\n    height = bar.get_height()\n    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n            f'{ratio:.2f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n\nplt.suptitle(f'{MODEL_ARCH.upper()}: Spectral Bias Analysis', fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\n{'='*70}\")\nprint(\"âœ… Spectral bias analysis complete!\")\nprint(f\"{'='*70}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 8: Spectral Bias Quantification\n\nCompute spectral bias metrics to quantify how well each model captures high-frequency content.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import torch.fft as fft\n\ndef compute_energy_spectrum(signal, n_bins=32):\n    \"\"\"\n    Compute time-averaged energy spectrum E(k) vs wavenumber.\n    \n    Args:\n        signal: [batch, channels, timesteps] tensor\n        n_bins: Number of wavenumber bins\n    \n    Returns:\n        wavenumbers: array of wavenumber bin centers\n        energy: E(k) energy spectrum values\n    \"\"\"\n    # FFT along time dimension\n    fft_signal = fft.rfft(signal, dim=-1)  # [B, C, freq]\n    \n    # Compute power: |FFT|^2\n    power = torch.abs(fft_signal) ** 2  # [B, C, freq]\n    \n    # Average over batch and channels\n    power_avg = power.mean(dim=(0, 1))  # [freq]\n    \n    # Create wavenumbers (k = 1, 2, ..., N)\n    n_freq = power_avg.shape[0]\n    wavenumbers = torch.arange(1, n_freq + 1, dtype=torch.float32)\n    \n    # Bin the frequencies into n_bins\n    bin_edges = torch.linspace(0, n_freq, n_bins + 1)\n    binned_energy = []\n    binned_k = []\n    \n    for i in range(n_bins):\n        start_idx = int(bin_edges[i])\n        end_idx = int(bin_edges[i + 1])\n        \n        if end_idx > start_idx:\n            # Average energy in this bin\n            bin_energy = power_avg[start_idx:end_idx].mean().item()\n            # Bin center wavenumber\n            bin_k = wavenumbers[start_idx:end_idx].mean().item()\n            \n            binned_energy.append(bin_energy)\n            binned_k.append(bin_k)\n    \n    return np.array(binned_k), np.array(binned_energy)\n\n\n# Get validation batch for energy spectrum analysis\nprint(\"Computing energy spectra for all trained models...\")\nval_input, val_target = next(iter(val_loader))\n\n# Move to device for inference\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nval_input = val_input.to(device)\nval_target = val_target.to(device)\n\n# Compute ground truth spectrum\nk_true, E_true = compute_energy_spectrum(val_target)\n\n# Compute spectrum for each trained model\nspectra = {}\nspectra['True'] = (k_true, E_true)\n\nfor loss_type in ['baseline', 'bsp', 'sa-bsp']:\n    key = f\"{MODEL_ARCH}_{loss_type}\"\n    model_trained = trained_models[key]\n    model_trained.eval()\n    model_trained.to(device)\n    \n    with torch.no_grad():\n        pred = model_trained(val_input)\n    \n    k_pred, E_pred = compute_energy_spectrum(pred)\n    spectra[f\"{MODEL_ARCH.upper()} + {loss_type.upper()}\"] = (k_pred, E_pred)\n    \n    print(f\"  âœ“ {loss_type.upper()} spectrum computed\")\n\n# Plot energy spectrum (like reference image)\nfig, ax = plt.subplots(figsize=(10, 8))\n\n# Plot ground truth (black solid line with higher zorder to be on top)\nax.loglog(k_true, E_true, 'k-', linewidth=3, label='True', zorder=10)\n\n# Plot model predictions with shaded confidence regions\ncolors_plot = {'baseline': '#1f77b4', 'bsp': '#ff7f0e', 'sa-bsp': '#2ca02c'}\n\nfor loss_type in ['baseline', 'bsp', 'sa-bsp']:\n    label_key = f\"{MODEL_ARCH.upper()} + {loss_type.upper()}\"\n    k, E = spectra[label_key]\n    \n    # Plot line\n    ax.loglog(k, E, color=colors_plot[loss_type], linewidth=2.5, \n             alpha=0.8, label=label_key, zorder=5)\n\n# Configure plot\nax.set_xlabel('Wavenumber', fontsize=14, fontweight='bold')\nax.set_ylabel('E(k)', fontsize=14, fontweight='bold')\nax.set_title(f'Time-Averaged Energy Spectrum Comparison\\\\n{MODEL_ARCH.upper()} Model', \n            fontsize=16, fontweight='bold')\nax.legend(fontsize=12, loc='best', framealpha=0.9)\nax.grid(True, alpha=0.3, which='both', linestyle='--')\n\n# Set nice axis limits\nax.set_xlim(k_true.min() * 0.9, k_true.max() * 1.1)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nâœ“ Energy spectrum plot complete\")\nprint(f\"  This plot shows spectral bias: deviation from ground truth at high wavenumbers\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 7: Spectral Bias Visualization (Energy Spectrum)\n\nVisualize E(k) vs wavenumber to identify spectral bias in trained models.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create multi-loss comparison plots\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Color scheme for loss types\ncolors = {'baseline': '#1f77b4', 'bsp': '#ff7f0e', 'sa-bsp': '#2ca02c'}\nlinestyles = {'baseline': '-', 'bsp': '--', 'sa-bsp': '-.'}\nmarkers = {'baseline': 'o', 'bsp': 's', 'sa-bsp': '^'}\n\nfor loss_type in ['baseline', 'bsp', 'sa-bsp']:\n    key = f\"{MODEL_ARCH}_{loss_type}\"\n    results = all_training_results[key]\n    \n    # Extract metrics\n    val_losses = [h['loss'] for h in results['val_history']]\n    val_field_errors = [h['field_error'] for h in results['val_history']]\n    val_spectrum_errors = [h['spectrum_error'] for h in results['val_history']]\n    epochs = range(1, len(val_losses) + 1)\n    \n    # Plot on all 3 axes\n    axes[0].plot(epochs, val_losses, label=loss_type.upper(), \n                color=colors[loss_type], linestyle=linestyles[loss_type],\n                linewidth=2, alpha=0.9, marker=markers[loss_type], markersize=4, markevery=5)\n    \n    axes[1].plot(epochs, val_field_errors, label=loss_type.upper(),\n                color=colors[loss_type], linestyle=linestyles[loss_type],\n                linewidth=2, alpha=0.9, marker=markers[loss_type], markersize=4, markevery=5)\n    \n    axes[2].plot(epochs, val_spectrum_errors, label=loss_type.upper(),\n                color=colors[loss_type], linestyle=linestyles[loss_type],\n                linewidth=2, alpha=0.9, marker=markers[loss_type], markersize=4, markevery=5)\n\n# Configure axes\naxes[0].set_xlabel('Epoch', fontsize=12)\naxes[0].set_ylabel('Validation Loss', fontsize=12)\naxes[0].set_title('Validation Loss Comparison', fontsize=14, fontweight='bold')\naxes[0].legend(fontsize=11, loc='best')\naxes[0].grid(True, alpha=0.3)\n\naxes[1].set_xlabel('Epoch', fontsize=12)\naxes[1].set_ylabel('Field Error', fontsize=12)\naxes[1].set_title('Field Error (Real Space)', fontsize=14, fontweight='bold')\naxes[1].legend(fontsize=11, loc='best')\naxes[1].grid(True, alpha=0.3)\n\naxes[2].set_xlabel('Epoch', fontsize=12)\naxes[2].set_ylabel('Spectrum Error', fontsize=12)\naxes[2].set_title('Spectrum Error (Frequency Space)', fontsize=14, fontweight='bold')\naxes[2].legend(fontsize=11, loc='best')\naxes[2].grid(True, alpha=0.3)\n\nplt.suptitle(f'{MODEL_ARCH.upper()}: Loss Function Comparison', \n             fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Print final metrics table\nprint(f\"\\n{'='*70}\")\nprint(\"Final Metrics Summary\")\nprint(f\"{'='*70}\")\nprint(f\"{'Loss Type':<15} {'Val Loss':<12} {'Field Error':<15} {'Spectrum Error':<15}\")\nprint(\"-\"*70)\n\nfor loss_type in ['baseline', 'bsp', 'sa-bsp']:\n    key = f\"{MODEL_ARCH}_{loss_type}\"\n    results = all_training_results[key]\n    final_val = results['val_history'][-1]\n    \n    print(f\"{loss_type.upper():<15} {final_val['loss']:<12.6f} \"\n          f\"{final_val['field_error']:<15.6f} {final_val['spectrum_error']:<15.6f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 6: Multi-Loss Training Comparison\n\nCompare training metrics across all 3 loss functions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Train with all 3 loss types sequentially\nloss_types_to_train = ['baseline', 'bsp', 'sa-bsp']\n\nfor LOSS_TYPE in loss_types_to_train:\n    print(f\"\\n{'='*70}\")\n    print(f\"Training {MODEL_ARCH.upper()} with {LOSS_TYPE.upper()} Loss\")\n    print(f\"{'='*70}\\n\")\n    \n    # Select loss configuration\n    selected_loss_config = loss_config_map[LOSS_TYPE]\n    print(f\"Loss config: {selected_loss_config.description}\")\n    \n    # Create loss function\n    criterion = create_loss(selected_loss_config)\n    print(f\"âœ“ Loss function created: {type(criterion).__name__}\")\n    \n    # Create FRESH model for this loss type (important!)\n    model_for_loss = create_model(MODEL_ARCH)\n    num_params = sum(p.numel() for p in model_for_loss.parameters() if p.requires_grad)\n    print(f\"âœ“ Fresh model created ({num_params:,} parameters)\")\n    \n    # Create training config\n    config = TrainingConfig(\n        num_epochs=50,\n        learning_rate=1e-3,\n        batch_size=BATCH_SIZE,\n        weight_decay=1e-4,\n        scheduler_type='cosine',\n        cosine_eta_min=1e-6,\n        eval_metrics=['field_error', 'spectrum_error'],\n        eval_frequency=1,\n        checkpoint_dir=f'checkpoints/{MODEL_ARCH}_{LOSS_TYPE}',\n        save_best=False,\n        save_latest=False,\n        device='cuda' if torch.cuda.is_available() else 'cpu',\n        num_workers=2,\n        verbose=True\n    )\n    \n    # Create trainer\n    trainer = SimpleTrainer(\n        model=model_for_loss,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        config=config,\n        loss_config=selected_loss_config,\n        experiment_name=f'{MODEL_ARCH}_{LOSS_TYPE}'\n    )\n    \n    print(f\"âœ“ Trainer initialized\")\n    print(f\"  Device: {trainer.device}\")\n    print(f\"  Optimizer: {type(trainer.optimizer).__name__}\")\n    \n    # Check for weight optimizer (SA-BSP only)\n    if LOSS_TYPE == 'sa-bsp':\n        if trainer.weight_optimizer is not None:\n            print(f\"  Weight optimizer: âœ“ Created for SA-BSP\")\n        else:\n            print(f\"  âš  WARNING: SA-BSP but no weight_optimizer!\")\n    \n    print(f\"\\nðŸš€ Starting training...\\\\n\")\n    \n    # Train\n    results = trainer.train()\n    \n    # Store results\n    key = f\"{MODEL_ARCH}_{LOSS_TYPE}\"\n    all_training_results[key] = results\n    all_trainers[key] = trainer\n    trained_models[key] = model_for_loss\n    \n    print(f\"\\nâœ… {LOSS_TYPE.upper()} training complete!\")\n    print(f\"   Best val loss: {results['best_val_loss']:.6f}\")\n    print(f\"   Final val loss: {results['val_history'][-1]['loss']:.6f}\")\n\nprint(f\"\\n{'='*70}\")\nprint(f\"ALL TRAINING COMPLETE!\")\nprint(f\"{'='*70}\")\nprint(f\"Trained {len(all_training_results)} models with different loss functions\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 5: Sequential Training with All Loss Types\n\nTrain the same model architecture with all 3 loss functions sequentially.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Import loss configurations\nfrom configs.loss_config import BASELINE_CONFIG, BSP_CONFIG, SA_BSP_CONFIG\nfrom src.core.evaluation.loss_factory import create_loss\n\n# Loss configuration map\nloss_config_map = {\n    'baseline': BASELINE_CONFIG,\n    'bsp': BSP_CONFIG,\n    'sa-bsp': SA_BSP_CONFIG\n}\n\n# Storage dictionaries for results from all loss types\nall_training_results = {}  # Key: f\"{MODEL_ARCH}_{loss_type}\"\nall_trainers = {}\ntrained_models = {}\n\nprint(\"âœ“ Storage initialized for multi-loss training\")\nprint(\"\\nWill train with 3 loss types:\")\nprint(\"  1. BASELINE:\", BASELINE_CONFIG.description)\nprint(\"  2. BSP:\", BSP_CONFIG.description)\nprint(\"  3. SA-BSP:\", SA_BSP_CONFIG.description)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 4: Initialize Results Storage\n\nWe'll train with all 3 loss types sequentially and store results for comparison.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4A: Import Loss Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.loss_config import BASELINE_CONFIG, BSP_CONFIG, SA_BSP_CONFIG\n",
    "from src.core.evaluation.loss_factory import create_loss\n",
    "\n",
    "print(\"âœ“ Loss configurations imported successfully\")\n",
    "print(\"\\nAvailable loss types:\")\n",
    "print(f\"  1. BASELINE: {BASELINE_CONFIG.description}\")\n",
    "print(f\"  2. BSP:      {BSP_CONFIG.description}\")\n",
    "print(f\"  3. SA-BSP:   {SA_BSP_CONFIG.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4B: Select Loss Type\n",
    "\n",
    "**Change `LOSS_TYPE` below to experiment:**\n",
    "- `'baseline'`: Standard Relative L2 loss (default)\n",
    "- `'bsp'`: Binned Spectral Power loss\n",
    "- `'sa-bsp'`: Self-Adaptive BSP with learnable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose loss type (CHANGE THIS TO EXPERIMENT)\n",
    "LOSS_TYPE = 'baseline'  # Options: 'baseline', 'bsp', 'sa-bsp'\n",
    "\n",
    "# Map to configuration\n",
    "loss_config_map = {\n",
    "    'baseline': BASELINE_CONFIG,\n",
    "    'bsp': BSP_CONFIG,\n",
    "    'sa-bsp': SA_BSP_CONFIG\n",
    "}\n",
    "\n",
    "if LOSS_TYPE not in loss_config_map:\n",
    "    raise ValueError(f\"Invalid LOSS_TYPE: '{LOSS_TYPE}'\")\n",
    "\n",
    "selected_loss_config = loss_config_map[LOSS_TYPE]\n",
    "\n",
    "print(f\"âœ“ Selected loss type: {LOSS_TYPE.upper()}\")\n",
    "print(f\"  Description: {selected_loss_config.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4C: Create and Validate Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss function\n",
    "criterion = create_loss(selected_loss_config)\n",
    "\n",
    "print(f\"âœ“ Loss function created: {type(criterion).__name__}\")\n",
    "\n",
    "# Validate with dummy tensors\n",
    "dummy_pred = torch.randn(4, 1, 1000)\n",
    "dummy_target = torch.randn(4, 1, 1000)\n",
    "\n",
    "test_loss = criterion(dummy_pred, dummy_target)\n",
    "print(f\"âœ“ Dummy loss value: {test_loss.item():.6f}\")\n",
    "print(f\"âœ“ Loss is finite: {torch.isfinite(test_loss).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4D: Test Loss on Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on real CDON data\n",
    "sample_batch_input, sample_batch_target = next(iter(train_loader))\n",
    "\n",
    "real_data_loss = criterion(sample_batch_input, sample_batch_target)\n",
    "\n",
    "print(f\"âœ“ Loss on real data: {real_data_loss.item():.6f}\")\n",
    "print(f\"âœ“ Loss is finite: {torch.isfinite(real_data_loss).item()}\")\n",
    "print(f\"âœ“ Ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TrainingConfig(\n",
    "    num_epochs=50,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    weight_decay=1e-4,\n",
    "    scheduler_type='cosine',\n",
    "    cosine_eta_min=1e-6,\n",
    "    eval_metrics=['field_error', 'spectrum_error'],\n",
    "    eval_frequency=1,\n",
    "    checkpoint_dir=f'checkpoints/{MODEL_ARCH}',\n",
    "    save_best=False,      # Disabled for debugging\n",
    "    save_latest=False,    # Disabled for debugging\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    num_workers=2,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Training configuration:\")\n",
    "print(f\"  Epochs: {config.num_epochs}\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")\n",
    "print(f\"  Device: {config.device}\")\n",
    "print(f\"  âš  Checkpointing: DISABLED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Create Trainer and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer with loss_config (required parameter)\n",
    "trainer = SimpleTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    config=config,\n",
    "    loss_config=selected_loss_config,  # Required parameter\n",
    "    experiment_name=f'{MODEL_ARCH}_{LOSS_TYPE}'\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Trainer initialized\")\n",
    "print(f\"  Device: {trainer.device}\")\n",
    "print(f\"  Optimizer: {type(trainer.optimizer).__name__}\")\n",
    "print(f\"  Loss: {type(trainer.criterion).__name__}\")\n",
    "\n",
    "# Check for weight optimizer (SA-BSP only)\n",
    "if LOSS_TYPE == 'sa-bsp':\n",
    "    if trainer.weight_optimizer is not None:\n",
    "        print(f\"  Weight optimizer: {type(trainer.weight_optimizer).__name__} âœ“\")\n",
    "    else:\n",
    "        print(f\"  âš  WARNING: SA-BSP selected but weight_optimizer is None!\")\n",
    "else:\n",
    "    print(f\"  Weight optimizer: None\")\n",
    "\n",
    "print(f\"\\nStarting training...\\n\")\n",
    "\n",
    "# Train\n",
    "results = trainer.train()\n",
    "\n",
    "print(f\"\\nâœ“ Training complete!\")\n",
    "print(f\"  Best val loss: {results['best_val_loss']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics\n",
    "train_losses = [h['loss'] for h in results['train_history']]\n",
    "val_losses = [h['loss'] for h in results['val_history']]\n",
    "val_field_errors = [h['field_error'] for h in results['val_history']]\n",
    "val_spectrum_errors = [h['spectrum_error'] for h in results['val_history']]\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "# Create plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: Loss\n",
    "axes[0].plot(epochs, train_losses, label='Train Loss', marker='o', markersize=3)\n",
    "axes[0].plot(epochs, val_losses, label='Val Loss', marker='s', markersize=3)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title(f'Training Loss ({LOSS_TYPE.upper()})')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Field Error\n",
    "axes[1].plot(epochs, val_field_errors, label='Val Field Error', marker='s', markersize=3, color='orange')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Field Error')\n",
    "axes[1].set_title('Field Error (Real Space)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Spectrum Error\n",
    "axes[2].plot(epochs, val_spectrum_errors, label='Val Spectrum Error', marker='s', markersize=3, color='green')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Spectrum Error')\n",
    "axes[2].set_title('Spectrum Error (Frequency Space)')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'{MODEL_ARCH.upper()} Training Results', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "print(f\"\\nFinal Metrics:\")\n",
    "print(f\"  Train Loss: {train_losses[-1]:.6f}\")\n",
    "print(f\"  Val Loss: {val_losses[-1]:.6f}\")\n",
    "print(f\"  Val Field Error: {val_field_errors[-1]:.6f}\")\n",
    "print(f\"  Val Spectrum Error: {val_spectrum_errors[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. âœ“ Loading real CDON data with proper normalization\n",
    "2. âœ“ Creating neural operator models (DeepONet, FNO, UNet)\n",
    "3. âœ“ **Configurable loss functions** (Baseline, BSP, SA-BSP)\n",
    "4. âœ“ Training with SimpleTrainer\n",
    "5. âœ“ Visualizing training metrics\n",
    "\n",
    "**Experiment with different configurations:**\n",
    "- **Cell 3**: Change `MODEL_ARCH` to try different models\n",
    "- **Cell 4B**: Change `LOSS_TYPE` to try different loss functions\n",
    "- **Cell 5**: Adjust hyperparameters (epochs, learning rate, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}