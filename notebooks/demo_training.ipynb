{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Operator Training Demo: CDON Dataset\n",
    "\n",
    "This notebook demonstrates end-to-end training of neural operator models (DeepONet, FNO, UNet) on the CDON dataset.\n",
    "\n",
    "**Features:**\n",
    "- Trains on **real CDON data**\n",
    "- **Sequential training with all 3 loss functions** (Baseline, BSP, SA-BSP)\n",
    "- **Multi-loss comparison plots** showing training metrics\n",
    "- **Energy spectrum visualization** (E(k) vs wavenumber) to identify spectral bias\n",
    "- **Spectral bias quantification** with metrics and comparison plots\n",
    "- Compatible with Google Colab\n",
    "\n",
    "**Models available:**\n",
    "- `deeponet`: Branch-trunk architecture with SIREN activation (~235K params)\n",
    "- `fno`: Fourier Neural Operator (~261K params)\n",
    "- `unet`: Encoder-decoder with skip connections (~249K params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup & Imports (Colab-Ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab setup\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure we're in /content\n",
    "try:\n",
    "    os.chdir('/content')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Clone repository if running in Colab\n",
    "repo_path = Path('/content/local')\n",
    "if not repo_path.exists():\n",
    "    print(\"ðŸ“¥ Cloning repository...\")\n",
    "    !git clone https://github.com/maximbeekenkamp/local.git\n",
    "    print(\"âœ… Repository cloned\")\n",
    "else:\n",
    "    print(\"âœ… Repository exists\")\n",
    "\n",
    "# Change to repo directory\n",
    "try:\n",
    "    os.chdir('/content/local')\n",
    "    print(f\"âœ… Changed to: {os.getcwd()}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Install dependencies\n",
    "print(\"\\nðŸ“¦ Installing dependencies...\")\n",
    "!pip install -r requirements.txt -q\n",
    "print(\"âœ… Dependencies installed\")\n",
    "\n",
    "# Standard imports\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Project imports\n",
    "from src.core.data_processing.cdon_dataset import CDONDataset\n",
    "from src.core.data_processing.cdon_transforms import CDONNormalization\n",
    "from src.core.models.model_factory import create_model\n",
    "from src.core.training.simple_trainer import SimpleTrainer\n",
    "from configs.training_config import TrainingConfig\n",
    "\n",
    "print(\"\\nâœ“ Imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Load Real CDON Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get project root\n",
    "project_root = Path.cwd()\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Data directory\n",
    "DATA_DIR = project_root / 'CDONData'\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "\n",
    "# Create normalization object (required by CDONDataset)\n",
    "stats_path = project_root / 'configs' / 'cdon_stats.json'\n",
    "print(f\"Loading stats from: {stats_path}\")\n",
    "normalizer = CDONNormalization(stats_path=str(stats_path))\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CDONDataset(\n",
    "    data_dir=str(DATA_DIR),\n",
    "    split='train',\n",
    "    normalize=normalizer\n",
    ")\n",
    "\n",
    "val_dataset = CDONDataset(\n",
    "    data_dir=str(DATA_DIR),\n",
    "    split='test',\n",
    "    normalize=normalizer\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Data loaded successfully\")\n",
    "print(f\"  Train samples: {len(train_dataset)}\")\n",
    "print(f\"  Val samples: {len(val_dataset)}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# Inspect a sample\n",
    "sample_input, sample_target = train_dataset[0]\n",
    "print(f\"\\nSample shapes:\")\n",
    "print(f\"  Input: {sample_input.shape}\")\n",
    "print(f\"  Target: {sample_target.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Choose Model Architecture\n",
    "\n",
    "**Change `MODEL_ARCH` to try different models:**\n",
    "- `'deeponet'`: Branch-trunk architecture with SIREN activation\n",
    "- `'fno'`: Fourier Neural Operator\n",
    "- `'unet'`: U-Net encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model architecture\n",
    "MODEL_ARCH = 'deeponet'  # Options: 'deeponet', 'fno', 'unet'\n",
    "\n",
    "model = create_model(MODEL_ARCH)\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"âœ“ Created {MODEL_ARCH.upper()} model\")\n",
    "print(f\"  Parameters: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Initialize Results Storage\n",
    "\n",
    "We'll train with all 3 loss types sequentially and store results for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import loss configurations\n",
    "from configs.loss_config import BASELINE_CONFIG, BSP_CONFIG, SA_BSP_CONFIG\n",
    "from src.core.evaluation.loss_factory import create_loss\n",
    "\n",
    "# Loss configuration map\n",
    "loss_config_map = {\n",
    "    'baseline': BASELINE_CONFIG,\n",
    "    'bsp': BSP_CONFIG,\n",
    "    'sa-bsp': SA_BSP_CONFIG\n",
    "}\n",
    "\n",
    "# Storage dictionaries for results from all loss types\n",
    "all_training_results = {}  # Key: f\"{MODEL_ARCH}_{loss_type}\"\n",
    "all_trainers = {}\n",
    "trained_models = {}\n",
    "\n",
    "print(\"âœ“ Storage initialized for multi-loss training\")\n",
    "print(\"\\nWill train with 3 loss types:\")\n",
    "print(\"  1. BASELINE:\", BASELINE_CONFIG.description)\n",
    "print(\"  2. BSP:\", BSP_CONFIG.description)\n",
    "print(\"  3. SA-BSP:\", SA_BSP_CONFIG.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Sequential Training with All Loss Types\n",
    "\n",
    "Train the same model architecture with all 3 loss functions sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with all 3 loss types sequentially\n",
    "loss_types_to_train = ['baseline', 'bsp', 'sa-bsp']\n",
    "\n",
    "for LOSS_TYPE in loss_types_to_train:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training {MODEL_ARCH.upper()} with {LOSS_TYPE.upper()} Loss\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Select loss configuration\n",
    "    selected_loss_config = loss_config_map[LOSS_TYPE]\n",
    "    print(f\"Loss config: {selected_loss_config.description}\")\n",
    "    \n",
    "    # Create loss function\n",
    "    criterion = create_loss(selected_loss_config)\n",
    "    print(f\"âœ“ Loss function created: {type(criterion).__name__}\")\n",
    "    \n",
    "    # Create FRESH model for this loss type (important!)\n",
    "    model_for_loss = create_model(MODEL_ARCH)\n",
    "    num_params = sum(p.numel() for p in model_for_loss.parameters() if p.requires_grad)\n",
    "    print(f\"âœ“ Fresh model created ({num_params:,} parameters)\")\n",
    "    \n",
    "    # Create training config\n",
    "    config = TrainingConfig(\n",
    "        num_epochs=50,\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        weight_decay=1e-4,\n",
    "        scheduler_type='cosine',\n",
    "        cosine_eta_min=1e-6,\n",
    "        eval_metrics=['field_error', 'spectrum_error'],\n",
    "        eval_frequency=1,\n",
    "        checkpoint_dir=f'checkpoints/{MODEL_ARCH}_{LOSS_TYPE}',\n",
    "        save_best=False,\n",
    "        save_latest=False,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        num_workers=2,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = SimpleTrainer(\n",
    "        model=model_for_loss,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        config=config,\n",
    "        loss_config=selected_loss_config,\n",
    "        experiment_name=f'{MODEL_ARCH}_{LOSS_TYPE}'\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ“ Trainer initialized\")\n",
    "    print(f\"  Device: {trainer.device}\")\n",
    "    print(f\"  Optimizer: {type(trainer.optimizer).__name__}\")\n",
    "    \n",
    "    # Check for weight optimizer (SA-BSP only)\n",
    "    if LOSS_TYPE == 'sa-bsp':\n",
    "        if trainer.weight_optimizer is not None:\n",
    "            print(f\"  Weight optimizer: âœ“ Created for SA-BSP\")\n",
    "        else:\n",
    "            print(f\"  âš  WARNING: SA-BSP but no weight_optimizer!\")\n",
    "    \n",
    "    print(f\"\\nðŸš€ Starting training...\\n\")\n",
    "    \n",
    "    # Train\n",
    "    results = trainer.train()\n",
    "    \n",
    "    # Store results\n",
    "    key = f\"{MODEL_ARCH}_{LOSS_TYPE}\"\n",
    "    all_training_results[key] = results\n",
    "    all_trainers[key] = trainer\n",
    "    trained_models[key] = model_for_loss\n",
    "    \n",
    "    print(f\"\\nâœ… {LOSS_TYPE.upper()} training complete!\")\n",
    "    print(f\"   Best val loss: {results['best_val_loss']:.6f}\")\n",
    "    print(f\"   Final val loss: {results['val_history'][-1]['loss']:.6f}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ALL TRAINING COMPLETE!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Trained {len(all_training_results)} models with different loss functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Multi-Loss Training Comparison\n",
    "\n",
    "Compare training metrics across all 3 loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-loss comparison plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Color scheme for loss types\n",
    "colors = {'baseline': '#1f77b4', 'bsp': '#ff7f0e', 'sa-bsp': '#2ca02c'}\n",
    "linestyles = {'baseline': '-', 'bsp': '--', 'sa-bsp': '-.'}\n",
    "markers = {'baseline': 'o', 'bsp': 's', 'sa-bsp': '^'}\n",
    "\n",
    "for loss_type in ['baseline', 'bsp', 'sa-bsp']:\n",
    "    key = f\"{MODEL_ARCH}_{loss_type}\"\n",
    "    results = all_training_results[key]\n",
    "    \n",
    "    # Extract metrics\n",
    "    val_losses = [h['loss'] for h in results['val_history']]\n",
    "    val_field_errors = [h['field_error'] for h in results['val_history']]\n",
    "    val_spectrum_errors = [h['spectrum_error'] for h in results['val_history']]\n",
    "    epochs = range(1, len(val_losses) + 1)\n",
    "    \n",
    "    # Plot on all 3 axes\n",
    "    axes[0].plot(epochs, val_losses, label=loss_type.upper(), \n",
    "                color=colors[loss_type], linestyle=linestyles[loss_type],\n",
    "                linewidth=2, alpha=0.9, marker=markers[loss_type], markersize=4, markevery=5)\n",
    "    \n",
    "    axes[1].plot(epochs, val_field_errors, label=loss_type.upper(),\n",
    "                color=colors[loss_type], linestyle=linestyles[loss_type],\n",
    "                linewidth=2, alpha=0.9, marker=markers[loss_type], markersize=4, markevery=5)\n",
    "    \n",
    "    axes[2].plot(epochs, val_spectrum_errors, label=loss_type.upper(),\n",
    "                color=colors[loss_type], linestyle=linestyles[loss_type],\n",
    "                linewidth=2, alpha=0.9, marker=markers[loss_type], markersize=4, markevery=5)\n",
    "\n",
    "# Configure axes\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Validation Loss', fontsize=12)\n",
    "axes[0].set_title('Validation Loss Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11, loc='best')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Field Error', fontsize=12)\n",
    "axes[1].set_title('Field Error (Real Space)', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11, loc='best')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].set_xlabel('Epoch', fontsize=12)\n",
    "axes[2].set_ylabel('Spectrum Error', fontsize=12)\n",
    "axes[2].set_title('Spectrum Error (Frequency Space)', fontsize=14, fontweight='bold')\n",
    "axes[2].legend(fontsize=11, loc='best')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'{MODEL_ARCH.upper()}: Loss Function Comparison', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics table\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Final Metrics Summary\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"{'Loss Type':<15} {'Val Loss':<12} {'Field Error':<15} {'Spectrum Error':<15}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for loss_type in ['baseline', 'bsp', 'sa-bsp']:\n",
    "    key = f\"{MODEL_ARCH}_{loss_type}\"\n",
    "    results = all_training_results[key]\n",
    "    final_val = results['val_history'][-1]\n",
    "    \n",
    "    print(f\"{loss_type.upper():<15} {final_val['loss']:<12.6f} \"\n",
    "          f\"{final_val['field_error']:<15.6f} {final_val['spectrum_error']:<15.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Spectral Bias Visualization (Energy Spectrum)\n",
    "\n",
    "Visualize E(k) vs wavenumber to identify spectral bias in trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.fft as fft\n",
    "\n",
    "def compute_energy_spectrum(signal, n_bins=32):\n",
    "    \"\"\"\n",
    "    Compute time-averaged energy spectrum E(k) vs wavenumber.\n",
    "    \n",
    "    Args:\n",
    "        signal: [batch, channels, timesteps] tensor\n",
    "        n_bins: Number of wavenumber bins\n",
    "    \n",
    "    Returns:\n",
    "        wavenumbers: array of wavenumber bin centers\n",
    "        energy: E(k) energy spectrum values\n",
    "    \"\"\"\n",
    "    # FFT along time dimension\n",
    "    fft_signal = fft.rfft(signal, dim=-1)  # [B, C, freq]\n",
    "    \n",
    "    # Compute power: |FFT|^2\n",
    "    power = torch.abs(fft_signal) ** 2  # [B, C, freq]\n",
    "    \n",
    "    # Average over batch and channels\n",
    "    power_avg = power.mean(dim=(0, 1))  # [freq]\n",
    "    \n",
    "    # Create wavenumbers (k = 1, 2, ..., N)\n",
    "    n_freq = power_avg.shape[0]\n",
    "    wavenumbers = torch.arange(1, n_freq + 1, dtype=torch.float32)\n",
    "    \n",
    "    # Bin the frequencies into n_bins\n",
    "    bin_edges = torch.linspace(0, n_freq, n_bins + 1)\n",
    "    binned_energy = []\n",
    "    binned_k = []\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        start_idx = int(bin_edges[i])\n",
    "        end_idx = int(bin_edges[i + 1])\n",
    "        \n",
    "        if end_idx > start_idx:\n",
    "            # Average energy in this bin\n",
    "            bin_energy = power_avg[start_idx:end_idx].mean().item()\n",
    "            # Bin center wavenumber\n",
    "            bin_k = wavenumbers[start_idx:end_idx].mean().item()\n",
    "            \n",
    "            binned_energy.append(bin_energy)\n",
    "            binned_k.append(bin_k)\n",
    "    \n",
    "    return np.array(binned_k), np.array(binned_energy)\n",
    "\n",
    "\n",
    "# Get validation batch for energy spectrum analysis\n",
    "print(\"Computing energy spectra for all trained models...\")\n",
    "val_input, val_target = next(iter(val_loader))\n",
    "\n",
    "# Move to device for inference\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "val_input = val_input.to(device)\n",
    "val_target = val_target.to(device)\n",
    "\n",
    "# Compute ground truth spectrum\n",
    "k_true, E_true = compute_energy_spectrum(val_target)\n",
    "\n",
    "# Compute spectrum for each trained model\n",
    "spectra = {}\n",
    "spectra['True'] = (k_true, E_true)\n",
    "\n",
    "for loss_type in ['baseline', 'bsp', 'sa-bsp']:\n",
    "    key = f\"{MODEL_ARCH}_{loss_type}\"\n",
    "    model_trained = trained_models[key]\n",
    "    model_trained.eval()\n",
    "    model_trained.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = model_trained(val_input)\n",
    "    \n",
    "    k_pred, E_pred = compute_energy_spectrum(pred)\n",
    "    spectra[f\"{MODEL_ARCH.upper()} + {loss_type.upper()}\"] = (k_pred, E_pred)\n",
    "    \n",
    "    print(f\"  âœ“ {loss_type.upper()} spectrum computed\")\n",
    "\n",
    "# Plot energy spectrum (like reference image)\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Plot ground truth (black solid line with higher zorder to be on top)\n",
    "ax.loglog(k_true, E_true, 'k-', linewidth=3, label='True', zorder=10)\n",
    "\n",
    "# Plot model predictions\n",
    "colors_plot = {'baseline': '#1f77b4', 'bsp': '#ff7f0e', 'sa-bsp': '#2ca02c'}\n",
    "\n",
    "for loss_type in ['baseline', 'bsp', 'sa-bsp']:\n",
    "    label_key = f\"{MODEL_ARCH.upper()} + {loss_type.upper()}\"\n",
    "    k, E = spectra[label_key]\n",
    "    \n",
    "    # Plot line\n",
    "    ax.loglog(k, E, color=colors_plot[loss_type], linewidth=2.5, \n",
    "             alpha=0.8, label=label_key, zorder=5)\n",
    "\n",
    "# Configure plot\n",
    "ax.set_xlabel('Wavenumber', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('E(k)', fontsize=14, fontweight='bold')\n",
    "ax.set_title(f'Time-Averaged Energy Spectrum Comparison\\n{MODEL_ARCH.upper()} Model', \n",
    "            fontsize=16, fontweight='bold')\n",
    "ax.legend(fontsize=12, loc='best', framealpha=0.9)\n",
    "ax.grid(True, alpha=0.3, which='both', linestyle='--')\n",
    "\n",
    "# Set nice axis limits\n",
    "ax.set_xlim(k_true.min() * 0.9, k_true.max() * 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ Energy spectrum plot complete\")\n",
    "print(f\"  This plot shows spectral bias: deviation from ground truth at high wavenumbers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Spectral Bias Quantification\n",
    "\n",
    "Compute spectral bias metrics to quantify how well each model captures high-frequency content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core.visualization.spectral_analysis import compute_spectral_bias_metric\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SPECTRAL BIAS METRICS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nQuantifies how well each model captures different frequency ranges.\")\n",
    "print(\"Spectral Bias Ratio = High Freq Error / Low Freq Error\")\n",
    "print(\"  - Ratio > 2.0: Significant spectral bias (struggles with high frequencies)\")\n",
    "print(\"  - Ratio > 1.5: Moderate spectral bias\")\n",
    "print(\"  - Ratio â‰¤ 1.5: Low spectral bias (captures frequencies well)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compute metrics for each trained model\n",
    "spectral_metrics = {}\n",
    "\n",
    "for loss_type in ['baseline', 'bsp', 'sa-bsp']:\n",
    "    key = f\"{MODEL_ARCH}_{loss_type}\"\n",
    "    model_trained = trained_models[key]\n",
    "    model_trained.eval()\n",
    "    model_trained.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = model_trained(val_input)\n",
    "    \n",
    "    metrics = compute_spectral_bias_metric(pred.cpu(), val_target.cpu(), n_bins=32)\n",
    "    spectral_metrics[loss_type] = metrics\n",
    "    \n",
    "    print(f\"\\n{MODEL_ARCH.upper()} + {loss_type.upper()} Loss:\")\n",
    "    print(f\"  Low frequency error:   {metrics['low_freq_error']:.6f}\")\n",
    "    print(f\"  Mid frequency error:   {metrics['mid_freq_error']:.6f}\")\n",
    "    print(f\"  High frequency error:  {metrics['high_freq_error']:.6f}\")\n",
    "    print(f\"  Spectral bias ratio:   {metrics['spectral_bias_ratio']:.4f}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    if metrics['spectral_bias_ratio'] > 2.0:\n",
    "        print(f\"  â†’ âš ï¸  SIGNIFICANT spectral bias detected!\")\n",
    "        print(f\"     Model struggles with high-frequency content\")\n",
    "    elif metrics['spectral_bias_ratio'] > 1.5:\n",
    "        print(f\"  â†’ âš¡ MODERATE spectral bias\")\n",
    "        print(f\"     Some difficulty with high frequencies\")\n",
    "    else:\n",
    "        print(f\"  â†’ âœ… LOW spectral bias\")\n",
    "        print(f\"     Model captures frequency content well\")\n",
    "\n",
    "# Create comparison visualization\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Spectral Bias Comparison\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar plot 1: Frequency errors\n",
    "x = np.arange(3)\n",
    "width = 0.25\n",
    "\n",
    "loss_types = ['baseline', 'bsp', 'sa-bsp']\n",
    "low_errors = [spectral_metrics[lt]['low_freq_error'] for lt in loss_types]\n",
    "mid_errors = [spectral_metrics[lt]['mid_freq_error'] for lt in loss_types]\n",
    "high_errors = [spectral_metrics[lt]['high_freq_error'] for lt in loss_types]\n",
    "\n",
    "ax1.bar(x - width, low_errors, width, label='Low Freq', color='#2ca02c', alpha=0.8)\n",
    "ax1.bar(x, mid_errors, width, label='Mid Freq', color='#ff7f0e', alpha=0.8)\n",
    "ax1.bar(x + width, high_errors, width, label='High Freq', color='#d62728', alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('Loss Type', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Frequency Error', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Frequency Range Errors', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([lt.upper() for lt in loss_types])\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Bar plot 2: Spectral bias ratio\n",
    "bias_ratios = [spectral_metrics[lt]['spectral_bias_ratio'] for lt in loss_types]\n",
    "colors_bars = [colors_plot[lt] for lt in loss_types]\n",
    "\n",
    "bars = ax2.bar(x, bias_ratios, color=colors_bars, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Add threshold lines\n",
    "ax2.axhline(y=2.0, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Significant bias threshold')\n",
    "ax2.axhline(y=1.5, color='orange', linestyle='--', linewidth=2, alpha=0.7, label='Moderate bias threshold')\n",
    "\n",
    "ax2.set_xlabel('Loss Type', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Spectral Bias Ratio', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Spectral Bias Ratio (High/Low)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([lt.upper() for lt in loss_types])\n",
    "ax2.legend(fontsize=10, loc='upper right')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, ratio) in enumerate(zip(bars, bias_ratios)):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "            f'{ratio:.2f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.suptitle(f'{MODEL_ARCH.upper()}: Spectral Bias Analysis', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"âœ… Spectral bias analysis complete!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. âœ“ Loading real CDON data with proper normalization\n",
    "2. âœ“ Creating neural operator models (DeepONet, FNO, UNet)\n",
    "3. âœ“ **Sequential training with all 3 loss functions** (Baseline, BSP, SA-BSP)\n",
    "4. âœ“ **Multi-loss comparison plots** showing training metrics\n",
    "5. âœ“ **Energy spectrum visualization** (E(k) vs wavenumber) to identify spectral bias\n",
    "6. âœ“ **Spectral bias quantification** with metrics and comparison plots\n",
    "\n",
    "**Key Results:**\n",
    "- All 3 loss types trained on the same model architecture\n",
    "- Direct comparison shows which loss function best mitigates spectral bias\n",
    "- Energy spectrum plot reveals how well each model captures high-frequency content\n",
    "- Quantitative metrics identify spectral bias ratio for each approach\n",
    "\n",
    "**Experiment with different configurations:**\n",
    "- **Cell 3**: Change `MODEL_ARCH` to try different models ('deeponet', 'fno', 'unet')\n",
    "- **Cell 5**: Adjust hyperparameters (epochs, learning rate, etc.) in TrainingConfig\n",
    "- Run all cells sequentially to train and compare all loss types automatically!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
